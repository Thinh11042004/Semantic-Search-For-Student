import os
import logging
import unicodedata  # h·ªó tr·ª£ chu·∫©n h√≥a ti·∫øng Vi·ªát kh√¥ng d·∫•u
from fastapi import FastAPI, Body, Query
from fastapi.responses import JSONResponse
from sentence_transformers import SentenceTransformer
import psycopg2
from PyPDF2 import PdfReader
import docx 
from typing import Union, List
import numpy as np
import re

# T·∫°o th∆∞ m·ª•c logs n·∫øu ch∆∞a c√≥
if not os.path.exists("logs"):
    os.makedirs("logs")

# Thi·∫øt l·∫≠p ghi log
logging.basicConfig(
    filename="logs/upload.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = FastAPI()
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  # 768 chi·ªÅu 

# K·∫øt n·ªëi PostgreSQL
conn = psycopg2.connect(
    host="semantic_search_db",  
    port=5432,
    user="admin",
    password="123456",
    database="StudentFormDB"
)
cursor = conn.cursor()

# Helper function ƒë·ªÉ ƒë·ªçc file PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text
    return text

# Helper function ƒë·ªÉ ƒë·ªçc file DOCX
def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

# Chu·∫©n h√≥a ti·∫øng Vi·ªát (b·ªè d·∫•u, lowercase)
def normalize_text(text):
    """
    ‚úÖ Chu·∫©n h√≥a ti·∫øng Vi·ªát: b·ªè d·∫•u, chuy·ªÉn v·ªÅ lowercase
    """
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    return text.lower()

# üîß M·ªöI: Mapping t·ª´ vi·∫øt t·∫Øt ho·∫∑c ti·∫øng Anh ph·ªï bi·∫øn
SYNONYM_MAP = {
    "hocbong": "h·ªçc b·ªïng",
    "hoclai": "h·ªçc l·∫°i",
    "nghihoc": "ngh·ªâ h·ªçc",
    "report": "t∆∞·ªùng tr√¨nh",
    "dt": "ƒëi·ªán tho·∫°i",
    "sv": "sinh vi√™n",
    "gv": "gi·∫£ng vi√™n",
    "cancel": "h·ªßy",
    "apply": "xin",
    "form": "bi·ªÉu m·∫´u",
}

# ===== C√ÅC H√ÄM H·ªñ TR·ª¢ =====

def cosine_similarity(a, b):
    """
    ‚úÖ T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 vector embedding
    """
    try:
        a = np.array(a, dtype=np.float32)
        b = np.array(eval(b), dtype=np.float32) if isinstance(b, str) else np.array(b, dtype=np.float32)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
    except Exception as e:
        logging.error(f"L·ªói t√≠nh cosine similarity: {e}")
        return 0.0

def preprocess_query(query):
    """
    ‚úÖ Ti·ªÅn x·ª≠ l√Ω truy v·∫•n: chu·∫©n h√≥a + m·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a + x·ª≠ l√Ω vi·∫øt li·ªÅn
    """
    # Chu·∫©n h√≥a kh√¥ng d·∫•u v√† lowercase
    clean_query = normalize_text(query)
    
    # Ki·ªÉm tra vi·∫øt li·ªÅn (tr∆∞·ªùng h·ª£p kh√¥ng c√≥ d·∫•u c√°ch)
    no_space_query = clean_query.replace(" ", "")
    
    # Dictionary m·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a v√† vi·∫øt t·∫Øt (b·ªï sung th√™m)
    expanded_synonyms = {
        # T·ª´ vi·∫øt t·∫Øt ph·ªï bi·∫øn
        "hocbong": "h·ªçc b·ªïng",
        "hoclai": "h·ªçc l·∫°i",
        "nghihoc": "ngh·ªâ h·ªçc",
        "chuyentruong": "chuy·ªÉn tr∆∞·ªùng",
        "thi": "k·ª≥ thi",
        "thilai": "thi l·∫°i",
        "vang": "v·∫Øng",
        "thoihoc": "th√¥i h·ªçc",
        "baoluu": "b·∫£o l∆∞u",
        "bangdiem": "b·∫£ng ƒëi·ªÉm",
        "giahan": "gia h·∫°n",
        "hoanthi": "ho√£n thi",
        "muon": "mu·ªôn",
        "donxin": "ƒë∆°n xin",
        
        # T·ª´ ti·∫øng Anh -> ti·∫øng Vi·ªát
        "report": "t∆∞·ªùng tr√¨nh b√°o c√°o",
        "cancel": "h·ªßy b·ªè",
        "application": "ƒë∆°n xin",
        "form": "bi·ªÉu m·∫´u ƒë∆°n",
        "leave": "ngh·ªâ",
        "absent": "v·∫Øng m·∫∑t",
        "transfer": "chuy·ªÉn",
        "scholarship": "h·ªçc b·ªïng",
        "transcript": "b·∫£ng ƒëi·ªÉm",
        "makeup": "b√π thi l√†m l·∫°i",
        "exam": "k·ª≥ thi",
        "fee": "l·ªá ph√≠",
        "tuition": "h·ªçc ph√≠",
        "withdraw": "r√∫t",
        "delay": "ho√£n tr√¨",
        "suspend": "ƒë√¨nh ch·ªâ t·∫°m ng∆∞ng",
        
        # Ng·ªØ c·∫£nh -> lo·∫°i ƒë∆°n
        "mu·ªën ngh·ªâ h·ªçc": "ƒë∆°n xin ngh·ªâ h·ªçc",
        "kh√¥ng ƒë·∫øn tr∆∞·ªùng": "ƒë∆°n xin ngh·ªâ h·ªçc",
        "qu√™n thi": "ƒë∆°n xin thi l·∫°i",
        "kh√¥ng ƒëi thi": "ƒë∆°n xin ho√£n thi",
        "kh√¥ng mu·ªën h·ªçc": "ƒë∆°n xin th√¥i h·ªçc",
        "chuy·ªÉn tr∆∞·ªùng": "ƒë∆°n xin chuy·ªÉn tr∆∞·ªùng",
        "ch·∫≠m n·ªôp": "ƒë∆°n xin gia h·∫°n",
        "n·ªôp mu·ªôn": "ƒë∆°n xin n·ªôp mu·ªôn",
        "v·∫Øng thi": "ƒë∆°n xin ho√£n thi",
        "h·ªçc ti·∫øp": "ƒë∆°n xin ti·∫øp t·ª•c h·ªçc",
        "t·∫°m d·ª´ng": "ƒë∆°n xin b·∫£o l∆∞u"
    }
    
    # Ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠ trong t·ª´ ƒëi·ªÉn m·ªü r·ªông
    for key, value in expanded_synonyms.items():
        # N·∫øu t·ª´ kh√≥a xu·∫•t hi·ªán trong truy v·∫•n ƒë√£ chu·∫©n h√≥a
        if key in clean_query or key in no_space_query:
            return value  # Tr·∫£ v·ªÅ d·∫°ng m·ªü r·ªông
    
    # N·∫øu kh√¥ng c√≥ m·ªü r·ªông n√†o ph√π h·ª£p, tr·∫£ v·ªÅ truy v·∫•n g·ªëc ƒë√£ chu·∫©n h√≥a
    return clean_query

def create_snippet(content, query_clean, max_length=300):
    """
    ‚úÖ T·∫°o ƒëo·∫°n tr√≠ch ph√π h·ª£p t·ª´ n·ªôi dung, ∆∞u ti√™n ph·∫ßn ch·ª©a t·ª´ kh√≥a
    """
    # T√°ch th√†nh c√°c c√¢u
    sentences = re.split(r'[.!?]\s+', content)
    
    # T√¨m c√°c c√¢u ch·ª©a t·ª´ kh√≥a query
    query_terms = [term for term in query_clean.split() if len(term) > 2]
    matching_sentences = []
    
    for sentence in sentences:
        sentence_clean = normalize_text(sentence)
        if any(term in sentence_clean for term in query_terms):
            matching_sentences.append(sentence)
    
    # N·∫øu t√¨m th·∫•y c√¢u kh·ªõp, gh√©p l·∫°i th√†nh ƒëo·∫°n tr√≠ch
    if matching_sentences:
        snippet = " ".join(matching_sentences)
        if len(snippet) > max_length:
            return snippet[:max_length] + "..."
        return snippet
    
    # N·∫øu kh√¥ng t√¨m th·∫•y c√¢u n√†o kh·ªõp, l·∫•y ƒëo·∫°n ƒë·∫ßu
    return content[:max_length] + "..."

def normalize_and_expand(text):
    """
    ‚úÖ Chu·∫©n h√≥a + x·ª≠ l√Ω vi·∫øt li·ªÅn + d·ªãch vi·∫øt t·∫Øt/ti·∫øng Anh
    """
    text = normalize_text(text.replace(" ", ""))
    return SYNONYM_MAP.get(text, text)

# H√†m ki·ªÉm tra xem t√™n file ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu ch∆∞a
def is_file_exists(title):  
    cursor.execute("SELECT COUNT(*) FROM forms WHERE title = %s", (title,))
    count = cursor.fetchone()[0]
    return count > 0

# H√†m x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file
def delete_duplicate_files():
    try:
        cursor.execute("""
            DELETE FROM forms
            WHERE ctid NOT IN (
                SELECT min(ctid)
                FROM documents
                GROUP BY title
            );
        """)
        conn.commit()
        logging.info("ƒê√£ x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file.")
    except Exception as e:
        logging.error(f"L·ªói khi x√≥a file tr√πng: {e}")
        conn.rollback()

# H√†m upload file (d√πng chung cho API v√† auto load)
def upload_file_to_db(file_path):
    title = os.path.basename(file_path)
    
    if is_file_exists(title):
        logging.info(f"File '{title}' ƒë√£ t·ªìn t·∫°i. X√≥a file tr√πng...")
        delete_duplicate_files()
    
    content = ""
    if file_path.endswith(".pdf"):
        content = extract_text_from_pdf(file_path)
    elif file_path.endswith(".docx"):
        content = extract_text_from_docx(file_path)
    else:
        logging.warning(f"B·ªè qua file kh√¥ng h·ªó tr·ª£: {file_path}")
        return {"error": "Unsupported file type"}
    
    embedding = model.encode(content).tolist()
    
    try:
        cursor.execute(
            "INSERT INTO forms (title, content, embedding) VALUES (%s, %s, %s)",
            (title, content, embedding)
        )
        conn.commit()
        logging.info(f"Uploaded: {title}")
        return {"status": "uploaded", "title": title}
    except Exception as e:
        logging.error(f"L·ªói khi upload file: {e}")
        conn.rollback()
        return {"error": f"Failed to upload file: {e}"}

#API/ Search
@app.get("/search")
def semantic_search(
    query: str = Query(..., description="C√¢u truy v·∫•n t√¨m ki·∫øm"),
    top_k: int = Query(5, description="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ mu·ªën tr·∫£ v·ªÅ"),
    min_score: float = Query(0.3, description="ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu ƒë·ªÉ tr·∫£ v·ªÅ k·∫øt qu·∫£")
):
    """
    üîç Semantic search n√¢ng cao k·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p:
    - So kh·ªõp t√™n file ch√≠nh x√°c
    - So kh·ªõp ti√™u ƒë·ªÅ bi·ªÉu m·∫´u (c√≥/kh√¥ng d·∫•u, vi·∫øt t·∫Øt)
    - So kh·ªõp n·ªôi dung bi·ªÉu m·∫´u
    - Hi·ªÉu ng·ªØ c·∫£nh truy v·∫•n b·∫±ng SBERT
    - X·ª≠ l√Ω vi·∫øt t·∫Øt, kh√¥ng d·∫•u, v√† ti·∫øng Anh
    """
    logging.info(f"üîç Semantic search: {query}")
    try:
        # Chu·∫©n b·ªã truy v·∫•n
        original_query = query  # L∆∞u truy v·∫•n g·ªëc
        query = query.strip()
        
        # N·∫øu query qu√° ng·∫Øn (√≠t h∆°n 2 k√Ω t·ª±), tr·∫£ v·ªÅ th√¥ng b√°o l·ªói ho·∫∑c d·ªØ li·ªáu r·ªóng
        if len(query) < 2:
            return {
                "query": original_query,
                "results": []
            }
        
        # T·∫°o c√°c bi·∫øn th·ªÉ c·ªßa query ƒë·ªÉ t√¨m ki·∫øm
        query_clean = normalize_text(query)  # Kh√¥ng d·∫•u
        query_no_space = normalize_text(query.replace(" ", ""))  # Kh√¥ng d·∫•u + kh√¥ng space
        query_expanded = preprocess_query(query)  # X·ª≠ l√Ω vi·∫øt t·∫Øt, m·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a
        
        # M√£ h√≥a truy v·∫•n th√†nh vector
        query_vec = model.encode(query).tolist()
        
        # L·∫•y to√†n b·ªô bi·ªÉu m·∫´u
        cursor.execute("SELECT id, title, content, embedding FROM forms")
        rows = cursor.fetchall()
        
        if not rows:
            return {
                "query": original_query,
                "results": []
            }
        
        results = []
        
        for _id, title, content, embedding in rows:
            # Chu·∫©n h√≥a d·ªØ li·ªáu
            title_clean = normalize_text(title)  # Ti√™u ƒë·ªÅ kh√¥ng d·∫•u
            title_no_space = normalize_text(title.replace(" ", ""))  # Ti√™u ƒë·ªÅ kh√¥ng d·∫•u + kh√¥ng space
            content_clean = normalize_text(content)  # N·ªôi dung kh√¥ng d·∫•u
            
            # T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a
            semantic_score = cosine_similarity(query_vec, embedding)
            
            # 1. Kh·ªõp t√™n file ch√≠nh x√°c
            if query_no_space == title_no_space:
                match_score = 1.0
                match_reason = "üéØ Kh·ªõp ch√≠nh x√°c v·ªõi t√™n file"
            
            # 2. Kh·ªõp t√™n file g·∫ßn ƒë√∫ng 
            elif query_no_space in title_no_space:
                ratio = len(query_no_space) / len(title_no_space)
                # N·∫øu query chi·∫øm ph·∫ßn l·ªõn t√™n file (>70%)
                if ratio > 0.7:  
                    match_score = 0.95
                    match_reason = "üéØ Kh·ªõp g·∫ßn ƒë√∫ng v·ªõi t√™n file"
                else:
                    match_score = 0.85
                    match_reason = "üìÑ T√™n file ch·ª©a t·ª´ kh√≥a t√¨m ki·∫øm"
            
            # 3. Kh·ªõp t·ª´ ch√≠nh x√°c trong ti√™u ƒë·ªÅ
            elif query_clean in title_clean.split():
                match_score = 0.9
                match_reason = "üìã Ti√™u ƒë·ªÅ ch·ª©a t·ª´ kh√≥a ch√≠nh x√°c"
            
            # 4. Kh·ªõp v·ªõi ti√™u ƒë·ªÅ (d·∫°ng m·ªü r·ªông)
            elif query_expanded in title_clean:
                match_score = 0.85
                match_reason = "üìã Ti√™u ƒë·ªÅ ch·ª©a t·ª´ ƒë·ªìng nghƒ©a/m·ªü r·ªông"
            
            # 5. Kh·ªõp m·ªôt ph·∫ßn v·ªõi ti√™u ƒë·ªÅ
            elif any(word in title_clean for word in query_clean.split() if len(word) > 2):
                # T√≠nh % t·ª´ quan tr·ªçng kh·ªõp v·ªõi ti√™u ƒë·ªÅ
                query_words = [w for w in query_clean.split() if len(w) > 2]
                title_words = title_clean.split()
                
                matching_words = sum(1 for w in query_words if any(w in t for t in title_words))
                word_match_ratio = matching_words / len(query_words) if query_words else 0
                
                match_score = 0.7 + (word_match_ratio * 0.2)  # 0.7-0.9 t√πy % t·ª´ kh·ªõp
                match_reason = "üìù M·ªôt s·ªë t·ª´ kh√≥a xu·∫•t hi·ªán trong ti√™u ƒë·ªÅ"
            
            # 6. Kh·ªõp n·ªôi dung vƒÉn b·∫£n
            elif any(kw in content_clean for kw in query_clean.split() if len(kw) > 2):
                # ƒê·∫øm s·ªë t·ª´ kh·ªõp trong n·ªôi dung
                query_words = [w for w in query_clean.split() if len(w) > 2]
                matching_words = sum(1 for w in query_words if w in content_clean)
                word_match_ratio = matching_words / len(query_words) if query_words else 0
                
                content_score = 0.6 + (word_match_ratio * 0.2)  # 0.6-0.8 t√πy % t·ª´ kh·ªõp
                
                # Tr∆∞·ªùng h·ª£p ng·ªØ nghƒ©a cao
                if semantic_score > 0.7:
                    match_score = max(content_score, semantic_score)
                    match_reason = "ü§ñ Ng·ªØ nghƒ©a SBERT + n·ªôi dung ch·ª©a t·ª´ kh√≥a"
                else:
                    match_score = content_score
                    match_reason = "üìö N·ªôi dung ch·ª©a t·ª´ kh√≥a t√¨m ki·∫øm"
            
            # 7. Kh·ªõp ng·ªØ nghƒ©a (SBERT)
            else:
                match_score = semantic_score
                match_reason = "ü§ñ Kh·ªõp ng·ªØ nghƒ©a (SBERT)"
            
            # K·∫øt h·ª£p ƒëi·ªÉm semantic v√† match ƒë·ªÉ c√≥ ƒëi·ªÉm cu·ªëi c√πng
            final_score = max(match_score, semantic_score * 0.95)
            
            # Ch·ªâ th√™m k·∫øt qu·∫£ n·∫øu ƒëi·ªÉm cao h∆°n ng∆∞·ª°ng
            if final_score >= min_score:
                # T·∫°o ƒëo·∫°n tr√≠ch ƒë·ªÉ hi·ªÉn th·ªã
                snippet = create_snippet(content, query_clean, max_length=300)
                
                results.append({
                    "id": _id,
                    "title": title,
                    "snippet": snippet,
                    "similarity_score": round(final_score, 4),
                    "reason": match_reason
                })
        
        # Tr·∫£ v·ªÅ top-k k·∫øt qu·∫£ theo ƒëi·ªÉm, ho·∫∑c √≠t h∆°n n·∫øu kh√¥ng ƒë·ªß k·∫øt qu·∫£ ƒë·∫°t ti√™u chu·∫©n
        sorted_results = sorted(results, key=lambda x: x["similarity_score"], reverse=True)
        
        # S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ ph·ª• thu·ªôc v√†o ch·∫•t l∆∞·ª£ng match
        # N·∫øu k·∫øt qu·∫£ ƒë·∫ßu ti√™n c√≥ ƒëi·ªÉm r·∫•t cao (>0.9), c√≥ th·ªÉ tr·∫£ v·ªÅ √≠t k·∫øt qu·∫£ h∆°n
        actual_top_k = top_k
        if sorted_results and sorted_results[0]["similarity_score"] > 0.9:
            # T√¨m ng∆∞·ª°ng ƒëi·ªÉm gi·∫£m m·∫°nh
            scores = [r["similarity_score"] for r in sorted_results[:top_k]]
            for i in range(1, len(scores)):
                if scores[i-1] - scores[i] > 0.3:  # ƒêi·ªÉm gi·∫£m ƒë·ªôt ng·ªôt
                    actual_top_k = i
                    break
        
        return {
            "query": original_query,
            "results": sorted_results[:actual_top_k] 
        }

    except Exception as e:
        logging.error(f"‚ùå L·ªói t√¨m ki·∫øm: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": "L·ªói h·ªá th·ªëng", "detail": str(e)}
        )



@app.post("/top-k")
def top_k_search(query: str = Body(...), k: int = Body(...)):
    logging.info(f"üîç ƒêang t√¨m ki·∫øm v·ªõi truy v·∫•n: {query}, Top {k} k·∫øt qu·∫£")
    try:
        query_embedding = model.encode(query).tolist()
        cursor.execute("""
            SELECT title, content
            FROM forms
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_embedding, k))
        results = cursor.fetchall()
        if results:
            return {
                "query": query,
                "top_k_results": [
                    {"title": result[0], "content": result[1][:500] + "..."} for result in results
                ]
            }
        else:
            return {
                "query": query,
                "message": f"Kh√¥ng t√¨m th·∫•y {k} bi·ªÉu m·∫´u ph√π h·ª£p."
            }
    except Exception as e:
        logging.error(f"L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm: {e}")
        cursor.execute("ROLLBACK;")
        return {"error": f"Top-K search failed: {e}"}

@app.post("/get-embedding")
def get_embedding(text: Union[str, List[str]] = Body(...)):
    try:
        if isinstance(text, str):
            text = [text]
        text = [t[:5000] for t in text]
        embeddings = model.encode(text).tolist()
        return {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings
        }
    except Exception as e:
        logging.error(f"L·ªói khi sinh embedding: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"L·ªói khi sinh embedding: {e}"}
        )