import os
import logging
import unicodedata  # há»— trá»£ chuáº©n hÃ³a tiáº¿ng Viá»‡t khÃ´ng dáº¥u
from fastapi import FastAPI, Body, Query
from fastapi.responses import JSONResponse
from sentence_transformers import SentenceTransformer
import psycopg2
from PyPDF2 import PdfReader
import docx 
from typing import Union, List
import numpy as np
import re
import difflib
from collections import Counter
import time  # ğŸ†• TÃCH Há»¢P: Äá»ƒ tÃ­nh thá»i gian xá»­ lÃ½ tá»« GocL4

# Táº¡o thÆ° má»¥c logs náº¿u chÆ°a cÃ³
if not os.path.exists("logs"):
    os.makedirs("logs")

# Thiáº¿t láº­p ghi log
logging.basicConfig(
    filename="logs/upload.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = FastAPI()
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  # 768 chiá»u 

# Káº¿t ná»‘i PostgreSQL
conn = psycopg2.connect(
    host="semantic_search_db",  
    port=5432,
    user="admin",
    password="123456",
    database="StudentFormDB"
)
cursor = conn.cursor()

# ===== CÃC HÃ€M Há»– TRá»¢ NÃ‚NG Cáº¤P HOÃ€N CHá»ˆNH =====

# ğŸ†• TÃCH Há»¢P GocL4: Chuáº©n hÃ³a tiáº¿ng Viá»‡t nÃ¢ng cao
def normalize_text(text):
    """
    ğŸ”§ TÃCH Há»¢P GocL4: Chuáº©n hÃ³a tiáº¿ng Viá»‡t toÃ n diá»‡n
    - Bá» dáº¥u vÃ  chuyá»ƒn vá» lowercase  
    - Xá»­ lÃ½ kÃ½ tá»± Ä‘áº·c biá»‡t thÃ´ng minh
    - Chuáº©n hÃ³a khoáº£ng tráº¯ng tá»‘i Æ°u
    """
    if not text or not isinstance(text, str):
        return ""
    
    # Chuáº©n hÃ³a Unicode NFD Ä‘á»ƒ tÃ¡ch dáº¥u
    text = unicodedata.normalize('NFD', text)
    # Loáº¡i bá» dáº¥u thanh (Combining marks)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    
    # XÃ³a cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘ vÃ  khoáº£ng tráº¯ng
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Chuáº©n hÃ³a khoáº£ng tráº¯ng vÃ  chuyá»ƒn vá» chá»¯ thÆ°á»ng
    text = re.sub(r'\s+', ' ', text.lower().strip())
    
    return text

# ğŸ†• TÃCH Há»¢P GocL4: SYNONYM_MAP má»Ÿ rá»™ng toÃ n diá»‡n
SYNONYM_MAP = {
    # === VIáº¾T Táº®T PHá»” BIáº¾N - TÃCH Há»¢P GocL4 ===
    "hocbong": "há»c bá»•ng",
    "hoclai": "há»c láº¡i", 
    "nghihoc": "nghá»‰ há»c",
    "thoihoc": "thÃ´i há»c rÃºt khá»i trÆ°á»ng",
    "chuyentruong": "chuyá»ƒn trÆ°á»ng",
    "chuyennganh": "chuyá»ƒn ngÃ nh chuyá»ƒn chuyÃªn ngÃ nh",
    "bangdiem": "báº£ng Ä‘iá»ƒm transcript",
    "donxin": "Ä‘Æ¡n xin Ä‘Æ¡n Ä‘á» nghá»‹",
    "baoluu": "báº£o lÆ°u táº¡m dá»«ng há»c",
    "giahan": "gia háº¡n hoÃ£n láº¡i",
    "hoanthi": "hoÃ£n thi dá»i lá»‹ch thi",
    "thilai": "thi láº¡i lÃ m láº¡i bÃ i thi",
    "vangmat": "váº¯ng máº·t absent",
    "muon": "muá»™n cháº­m trá»…",
    "nopmuon": "ná»™p muá»™n submit late",
    "tamnghi": "táº¡m nghá»‰ nghá»‰ táº¡m thá»i",
    "tieptuc": "tiáº¿p tá»¥c continue",
    "rut": "rÃºt withdraw",
    "huy": "há»§y cancel",
    "dinhchi": "Ä‘Ã¬nh chá»‰ suspend",
    "caplai": "cáº¥p láº¡i reissue",
    "xacnhan": "xÃ¡c nháº­n confirm",
    "khentuong": "khen thÆ°á»Ÿng",
    "kyluat": "ká»· luáº­t",
    "phoihop": "phá»‘i há»£p",
    
    # === TIáº¾NG ANH -> TIáº¾NG VIá»†T - TÃCH Há»¢P GocL4 ===
    "application": "Ä‘Æ¡n xin Ä‘Æ¡n Ä‘á» nghá»‹",
    "form": "biá»ƒu máº«u Ä‘Æ¡n",
    "report": "bÃ¡o cÃ¡o tÆ°á»ng trÃ¬nh",
    "cancel": "há»§y bá»",
    "leave": "nghá»‰ phÃ©p",
    "absent": "váº¯ng máº·t",
    "transfer": "chuyá»ƒn Ä‘á»•i",
    "scholarship": "há»c bá»•ng",
    "transcript": "báº£ng Ä‘iá»ƒm",
    "makeup": "bÃ¹ thi lÃ m láº¡i",
    "exam": "ká»³ thi kiá»ƒm tra",
    "fee": "lá»‡ phÃ­",
    "tuition": "há»c phÃ­",
    "withdraw": "rÃºt khá»i",
    "delay": "hoÃ£n láº¡i",
    "suspend": "Ä‘Ã¬nh chá»‰ táº¡m ngÆ°ng",
    "extension": "gia háº¡n",
    "postpone": "hoÃ£n láº¡i",
    "reissue": "cáº¥p láº¡i",
    "confirm": "xÃ¡c nháº­n",
    "continue": "tiáº¿p tá»¥c",
    "temporary": "táº¡m thá»i",
    "permanent": "vÄ©nh viá»…n",
    
    # === NGá»® Cáº¢NH VÃ€ Ã Äá»ŠNH - TÃCH Há»¢P GocL4 ===
    "khongmuonhoc": "thÃ´i há»c rÃºt khá»i trÆ°á»ng",
    "muonnghihoc": "nghá»‰ há»c táº¡m dá»«ng há»c", 
    "khongdentruong": "nghá»‰ há»c váº¯ng máº·t",
    "quenthi": "thi láº¡i hoÃ£n thi bÃ¹ thi",
    "khongdithi": "hoÃ£n thi váº¯ng thi",
    "vangthi": "hoÃ£n thi bÃ¹ thi váº¯ng thi",
    "muonchuyentruong": "chuyá»ƒn trÆ°á»ng",
    "chuyendi": "chuyá»ƒn trÆ°á»ng chuyá»ƒn ngÃ nh chuyá»ƒn Ä‘iá»ƒm",
    "chamnop": "gia háº¡n ná»™p muá»™n",
    "nopmuon": "ná»™p muá»™n gia háº¡n",
    "hoctiep": "tiáº¿p tá»¥c há»c báº£o lÆ°u",
    "tamdung": "báº£o lÆ°u táº¡m nghá»‰",
    "tamnghi": "báº£o lÆ°u nghá»‰ há»c",
    "di": "Ä‘iá»ƒm chuyá»ƒn Ä‘i",
    "diem": "báº£ng Ä‘iá»ƒm chuyá»ƒn Ä‘iá»ƒm",
    "hocthem": "há»c thÃªm song song",
    "haichuongtrinh": "há»c song song parallel",
    "caplai": "cáº¥p láº¡i reissue",
    "xacnhan": "xÃ¡c nháº­n confirm",
    
    # === Cáº¢M XÃšC VÃ€ HOÃ€N Cáº¢NH - TÃCH Há»¢P GocL4 ===
    "khongmuon": "thÃ´i há»c chuyá»ƒn trÆ°á»ng nghá»‰ há»c",
    "chan": "thÃ´i há»c chuyá»ƒn ngÃ nh chuyá»ƒn trÆ°á»ng", 
    "metmoi": "nghá»‰ há»c báº£o lÆ°u",
    "khokhan": "há»c bá»•ng gia háº¡n hoÃ£n",
    "benh": "nghá»‰ há»c hoÃ£n thi báº£o lÆ°u",
    "giadinh": "nghá»‰ há»c báº£o lÆ°u chuyá»ƒn trÆ°á»ng",
    "muonhoc": "tiáº¿p tá»¥c há»c thÃªm chuyá»ƒn ngÃ nh",
    "can": "há»c bá»•ng xÃ¡c nháº­n cáº¥p láº¡i",
    "thieu": "cáº¥p láº¡i bá»• sung",
    
    # === VIáº¾T LIá»€N VÃ€ VIáº¾T Táº®T Äáº¶C BIá»†T - TÃCH Há»¢P GocL4 ===
    "donxinhepnghihoc": "Ä‘Æ¡n xin nghá»‰ há»c",
    "donxinthoihoc": "Ä‘Æ¡n xin thÃ´i há»c",
    "donxinchuyentruong": "Ä‘Æ¡n xin chuyá»ƒn trÆ°á»ng",
    "donxinchuyennganh": "Ä‘Æ¡n xin chuyá»ƒn ngÃ nh",
    "donxinhocbong": "Ä‘Æ¡n xin há»c bá»•ng",
    "donxinbaoluu": "Ä‘Æ¡n xin báº£o lÆ°u",
    "donxingiahan": "Ä‘Æ¡n xin gia háº¡n",
    "bangdiemtotnghiep": "báº£ng Ä‘iá»ƒm tá»‘t nghiá»‡p",
    "xacnhanhocsinh": "xÃ¡c nháº­n há»c sinh",
    "xacnhansinhvien": "xÃ¡c nháº­n sinh viÃªn",
    
    # === GIá»® NGUYÃŠN Tá»ª Cl5Sonnet ===
    "chuyen": "chuyá»ƒn",
    "chuyen di": "chuyá»ƒn Ä‘i chuyá»ƒn trÆ°á»ng chuyá»ƒn ngÃ nh chuyá»ƒn Ä‘iá»ƒm",
    "chuyendi": "chuyá»ƒn Ä‘i chuyá»ƒn trÆ°á»ng chuyá»ƒn ngÃ nh",
    "khongmuonhocnua": "thÃ´i há»c chuyá»ƒn trÆ°á»ng",
    "muonhocnganhmoi": "chuyá»ƒn ngÃ nh há»c thÃªm",
    "khongcodienthoai": "cáº­p nháº­t thÃ´ng tin liÃªn láº¡c",
    "matdienthoai": "cáº­p nháº­t thÃ´ng tin",
    "doidiachi": "cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n",
    "hocthem": "há»c hai chÆ°Æ¡ng trÃ¬nh song song",
    "hocthemnganh": "há»c hai chÆ°Æ¡ng trÃ¬nh song song",
    "khongthichnganhnaynua": "chuyá»ƒn ngÃ nh",
    "muonchuyenkhoa": "chuyá»ƒn ngÃ nh chuyá»ƒn khoa",
    "bangdiemtotnghhiep": "báº£ng Ä‘iá»ƒm tá»‘t nghiá»‡p",
    "hockhongduoc": "há»c láº¡i bá»• tÃºc",
    "truotmon": "há»c láº¡i thi láº¡i",
    "ketmon": "káº¿t thÃºc há»c pháº§n",
    "dangkymon": "Ä‘Äƒng kÃ½ há»c pháº§n",
    "huymon": "há»§y Ä‘Äƒng kÃ½ há»c pháº§n",
}

# ğŸ†• TÃCH Há»¢P GocL4: Báº£n Ä‘á»“ ngá»¯ cáº£nh cáº£m xÃºc/Ã½ Ä‘á»‹nh
CONTEXT_INTENT_MAP = {
    # === Cáº¢M XÃšC TIÃŠU Cá»°C ===
    "khong muon": ["thÃ´i há»c", "chuyá»ƒn trÆ°á»ng", "nghá»‰ há»c", "rÃºt khá»i"],
    "khong con": ["thÃ´i há»c", "chuyá»ƒn trÆ°á»ng", "nghá»‰ há»c"],
    "chan": ["thÃ´i há»c", "chuyá»ƒn ngÃ nh", "chuyá»ƒn trÆ°á»ng", "nghá»‰ há»c"], 
    "met moi": ["nghá»‰ há»c", "báº£o lÆ°u", "táº¡m nghi"],
    "kho khan": ["há»c bá»•ng", "gia háº¡n", "hoÃ£n", "há»— trá»£"],
    "benh tat": ["nghá»‰ há»c", "hoÃ£n thi", "báº£o lÆ°u", "gia háº¡n"],
    "gia dinh": ["nghá»‰ há»c", "báº£o lÆ°u", "chuyá»ƒn trÆ°á»ng", "hoÃ£n"],
    "khong du": ["há»c bá»•ng", "gia háº¡n", "há»— trá»£"],
    "qua kho": ["chuyá»ƒn ngÃ nh", "báº£o lÆ°u", "nghá»‰ há»c"],
    
    # === Ã Äá»ŠNH TÃCH Cá»°C ===
    "muon hoc": ["tiáº¿p tá»¥c", "há»c thÃªm", "chuyá»ƒn ngÃ nh", "song song"],
    "can": ["há»c bá»•ng", "xÃ¡c nháº­n", "cáº¥p láº¡i", "há»— trá»£"],
    "thieu": ["cáº¥p láº¡i", "bá»• sung", "xÃ¡c nháº­n"],
    "muon co": ["xÃ¡c nháº­n", "cáº¥p láº¡i", "báº£ng Ä‘iá»ƒm"],
    "xin": ["Ä‘Æ¡n xin", "Ä‘á» nghá»‹", "yÃªu cáº§u"],
    "mong": ["Ä‘á» nghá»‹", "xin", "yÃªu cáº§u"],
    
    # === HOÃ€N Cáº¢NH Äáº¶C BIá»†T ===
    "chuyen di": ["chuyá»ƒn trÆ°á»ng", "chuyá»ƒn ngÃ nh", "chuyá»ƒn Ä‘iá»ƒm"],
    "di lam": ["nghá»‰ há»c", "báº£o lÆ°u", "há»c táº¡i chá»©c"],
    "ket hon": ["nghá»‰ há»c", "báº£o lÆ°u", "chuyá»ƒn trÆ°á»ng"],
    "sinh con": ["nghá»‰ há»c", "báº£o lÆ°u", "táº¡m ngÆ°ng"],
    "di nuoc ngoai": ["chuyá»ƒn trÆ°á»ng", "báº£o lÆ°u", "nghá»‰ há»c"],
    "khong du tien": ["há»c bá»•ng", "gia háº¡n", "hoÃ£n há»c phÃ­"],
}

# ğŸ†• TÃCH Há»¢P GocL4: PhÃ¢n tÃ­ch tá»« khÃ³a quan trá»ng
def extract_keywords(text, min_length=2):
    """
    TrÃ­ch xuáº¥t tá»« khÃ³a quan trá»ng tá»« vÄƒn báº£n, loáº¡i bá» tá»« dá»«ng tiáº¿ng Viá»‡t
    """
    if not text:
        return []
    
    # Loáº¡i bá» tá»« dá»«ng tiáº¿ng Viá»‡t phá»• biáº¿n
    stop_words = {
        'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'vá»›i', 'Ä‘á»ƒ', 'cho', 'vá»', 
        'khi', 'Ä‘Ã£', 'sáº½', 'nÃ y', 'Ä‘Ã³', 'cÃ¡c', 'má»™t', 'nhá»¯ng', 'tá»«', 'táº¡i',
        'bá»Ÿi', 'theo', 'nhÆ°', 'Ä‘áº¿n', 'trÃªn', 'dÆ°á»›i', 'giá»¯a', 'sau', 'trÆ°á»›c',
        'náº¿u', 'mÃ ', 'thÃ¬', 'hoáº·c', 'nhÆ°ng', 'vÃ¬', 'do', 'nÃªn', 'hay'
    }
    
    words = normalize_text(text).split()
    keywords = [word for word in words 
                if len(word) >= min_length and word not in stop_words]
    return keywords

# ğŸ†• TÃCH Há»¢P GocL4: PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m
def analyze_search_intent(query):
    """
    PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m Ä‘á»ƒ Ä‘iá»u chá»‰nh thuáº­t toÃ¡n:
    - 'filename': TÃ¬m theo tÃªn file chÃ­nh xÃ¡c
    - 'title': TÃ¬m theo tiÃªu Ä‘á» biá»ƒu máº«u  
    - 'content': TÃ¬m theo ná»™i dung
    - 'semantic': TÃ¬m theo ngá»¯ nghÄ©a vÃ  Ã½ Ä‘á»‹nh
    - 'mixed': TÃ¬m tá»•ng há»£p
    """
    if not query:
        return 'mixed'
    
    query_clean = normalize_text(query)
    query_lower = query.lower()
    
    # === PHÃ‚N TÃCH CÃC PATTERN Äáº¶C TRÆ¯NG ===
    
    # 1. TÃ¬m theo tÃªn file (cÃ³ Ä‘uÃ´i file hoáº·c tÃªn file cá»¥ thá»ƒ)
    file_extensions = ['.docx', '.pdf', '.doc', '.txt']
    if any(ext in query_lower for ext in file_extensions):
        return 'filename'
    
    # TÃªn file pattern (chá»©a "hutech" + chuá»—i liá»n khÃ´ng dáº¥u)
    if re.search(r'hutech\w{5,}', query_clean):
        return 'filename'
    
    # 2. TÃ¬m theo tiÃªu Ä‘á» (cÃ³ tá»« khÃ³a "Ä‘Æ¡n", "biá»ƒu máº«u")
    title_indicators = ['don', 'bieu mau', 'form', 'application']
    if any(indicator in query_clean for indicator in title_indicators):
        return 'title'
    
    # 3. TÃ¬m theo ngá»¯ nghÄ©a (cÃ¢u dÃ i, cÃ³ cáº£m xÃºc, Ã½ Ä‘á»‹nh)
    semantic_indicators = list(CONTEXT_INTENT_MAP.keys())
    if any(indicator in query_clean for indicator in semantic_indicators):
        return 'semantic'
    
    # CÃ¢u dÃ i (>6 tá»«) thÆ°á»ng lÃ  tÃ¬m kiáº¿m ngá»¯ nghÄ©a
    if len(query_clean.split()) > 6:
        return 'semantic'
    
    # 4. TÃ¬m theo ná»™i dung (tá»« khÃ³a cá»¥ thá»ƒ ngáº¯n gá»n)
    if len(query_clean.split()) <= 3 and not any(indicator in query_clean for indicator in title_indicators):
        return 'content'
    
    # Máº·c Ä‘á»‹nh: tÃ¬m tá»•ng há»£p
    return 'mixed'

# ğŸ†• TÃCH Há»¢P GocL4: Xá»­ lÃ½ tÃ¬m kiáº¿m real-time
def should_return_results(query, min_length=2):
    """
    Quyáº¿t Ä‘á»‹nh cÃ³ nÃªn tráº£ káº¿t quáº£ ngay hay khÃ´ng (cho real-time search)
    """
    if not query:
        return False, "empty_query"
    
    query_clean = query.strip()
    
    # QuÃ¡ ngáº¯n
    if len(query_clean) < min_length:
        return False, "too_short"
    
    # Chá»‰ cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t
    if not re.search(r'[a-zA-Z0-9Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', query_clean):
        return False, "no_meaningful_chars"
    
    return True, "valid"

# TÃ­nh toÃ¡n cosine similarity - GIá»® NGUYÃŠN Cl5Sonnet + Cáº¢I TIáº¾N
def cosine_similarity(a, b):
    """
    ğŸ”§ NÃ‚NG Cáº¤P: TÃ­nh toÃ¡n cosine similarity an toÃ n vÃ  chÃ­nh xÃ¡c
    - Xá»­ lÃ½ lá»—i robust tá»« Cl5Sonnet
    - Chuáº©n hÃ³a vector Ä‘áº§u vÃ o  
    - Kiá»ƒm tra edge cases
    """
    try:
        # Chuyá»ƒn Ä‘á»•i sang numpy array vá»›i dtype nháº¥t quÃ¡n
        a = np.array(a, dtype=np.float32)
        
        if isinstance(b, str):
            # Náº¿u b lÃ  string (tá»« database), parse nÃ³
            b = np.array(eval(b), dtype=np.float32)
        else:
            b = np.array(b, dtype=np.float32)
        
        # Kiá»ƒm tra kÃ­ch thÆ°á»›c vector
        if a.shape != b.shape:
            logging.warning(f"Vector size mismatch: {a.shape} vs {b.shape}")
            return 0.0
        
        # TÃ­nh norm
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        
        # Kiá»ƒm tra vector zero
        if norm_a == 0 or norm_b == 0:
            return 0.0
            
        # TÃ­nh cosine similarity
        similarity = float(np.dot(a, b) / (norm_a * norm_b))
        
        # Äáº£m báº£o káº¿t quáº£ trong khoáº£ng [-1, 1]
        similarity = max(-1.0, min(1.0, similarity))
        
        return similarity
        
    except Exception as e:
        logging.error(f"Lá»—i tÃ­nh cosine similarity: {e}")
        return 0.0

# ğŸ†• TÃCH Há»¢P HOÃ€N CHá»ˆNH: Tiá»n xá»­ lÃ½ truy váº¥n nÃ¢ng cao
def preprocess_query(query):
    """
    ğŸš€ TÃCH Há»¢P HOÃ€N CHá»ˆNH GocL4 + Cl5Sonnet: Xá»­ lÃ½ truy váº¥n thÃ´ng minh Ä‘a táº§ng
    
    LOGIC Xá»¬ LÃ TÃCH Há»¢P:
    1. PhÃ¢n tÃ­ch cáº¥u trÃºc cÃ¢u truy váº¥n (viáº¿t liá»n, viáº¿t táº¯t, ngá»¯ cáº£nh) - GocL4
    2. Ãp dá»¥ng mapping Ä‘á»“ng nghÄ©a thÃ´ng minh - GocL4 + Cl5Sonnet
    3. Má»Ÿ rá»™ng ngá»¯ cáº£nh dá»±a trÃªn Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng - GocL4
    4. Xá»­ lÃ½ Ä‘áº·c biá»‡t cho trÆ°á»ng há»£p phá»©c táº¡p - GocL4
    5. Táº¡o nhiá»u biáº¿n thá»ƒ truy váº¥n Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c - Cl5Sonnet
    """
    if not query or not isinstance(query, str):
        return ""
    
    original_query = query.strip()
    clean_query = normalize_text(original_query)
    no_space_query = clean_query.replace(" ", "")
    
    expanded_terms = []
    
    # === BÆ¯á»šC 1: KIá»‚M TRA MAPPING TRá»°C TIáº¾P (GocL4) ===   
    # Kiá»ƒm tra query viáº¿t liá»n trÆ°á»›c
    if no_space_query in SYNONYM_MAP:
        expanded_terms.extend(SYNONYM_MAP[no_space_query].split())
        logging.info(f"ğŸ¯ TÃ¬m tháº¥y mapping viáº¿t liá»n: {no_space_query} -> {SYNONYM_MAP[no_space_query]}")
    
    # Kiá»ƒm tra query cÃ³ khoáº£ng cÃ¡ch
    elif clean_query in SYNONYM_MAP:
        expanded_terms.extend(SYNONYM_MAP[clean_query].split())
        logging.info(f"ğŸ¯ TÃ¬m tháº¥y mapping trá»±c tiáº¿p: {clean_query} -> {SYNONYM_MAP[clean_query]}")
    
    # === BÆ¯á»šC 2: PHÃ‚N TÃCH Tá»ªNG Tá»ª (Cl5Sonnet + GocL4) ===
    query_words = clean_query.split()
    for word in query_words:
        if len(word) > 1:  # Bá» qua tá»« quÃ¡ ngáº¯n
            if word in SYNONYM_MAP:
                expansion = SYNONYM_MAP[word]
                expanded_terms.extend(expansion.split())
                logging.info(f"ğŸ“ Má»Ÿ rá»™ng tá»«: {word} -> {expansion}")
            else:
                expanded_terms.append(word)
        else:
            expanded_terms.append(word)
    
    # === BÆ¯á»šC 3: PHÃ‚N TÃCH NGá»® Cáº¢NH VÃ€ Ã Äá»ŠNH (GocL4) ===
    intent_keywords = []
    context_found = []
    
    for intent_phrase, related_forms in CONTEXT_INTENT_MAP.items():
        if intent_phrase in clean_query:
            intent_keywords.extend(related_forms)
            context_found.append(intent_phrase)
            logging.info(f"ğŸ§  PhÃ¡t hiá»‡n ngá»¯ cáº£nh: '{intent_phrase}' -> {related_forms}")
    
    # === BÆ¯á»šC 4: Xá»¬ LÃ Äáº¶C BIá»†T CHO CÃC TRÆ¯á»œNG Há»¢P PHá»¨C Táº P (GocL4) ===
    
    # Xá»­ lÃ½ "di" cÃ³ thá»ƒ lÃ  "Ä‘i" hoáº·c liÃªn quan Ä‘áº¿n "Ä‘iá»ƒm"
    if "di" in query_words:
        # Kiá»ƒm tra ngá»¯ cáº£nh xung quanh
        di_index = query_words.index("di")
        context_words = []
        
        # Láº¥y tá»« trÆ°á»›c vÃ  sau "di"
        if di_index > 0:
            context_words.append(query_words[di_index - 1])
        if di_index < len(query_words) - 1:
            context_words.append(query_words[di_index + 1])
        
        # Náº¿u cÃ³ tá»« liÃªn quan Ä‘áº¿n há»c táº­p -> cÃ³ thá»ƒ lÃ  "Ä‘iá»ƒm"
        academic_words = ["chuyen", "bang", "ket", "qua", "hoc"]
        if any(word in " ".join(context_words) for word in academic_words):
            intent_keywords.extend(["Ä‘iá»ƒm", "báº£ng Ä‘iá»ƒm", "chuyá»ƒn Ä‘iá»ƒm"])
            logging.info(f"ğŸ¯ PhÃ¢n tÃ­ch 'di' trong ngá»¯ cáº£nh há»c táº­p -> thÃªm 'Ä‘iá»ƒm'")
    
    # === BÆ¯á»šC 5: Táº O QUERY CUá»I CÃ™NG (Cl5Sonnet + GocL4) ===
    
    # Káº¿t há»£p táº¥t cáº£ cÃ¡c tá»« khÃ³a vÃ  loáº¡i bá» trÃ¹ng láº·p
    all_terms = expanded_terms + intent_keywords + query_words
    unique_terms = []
    seen = set()
    
    for term in all_terms:
        term_clean = term.strip().lower()
        if term_clean and term_clean not in seen and len(term_clean) > 1:
            unique_terms.append(term_clean)
            seen.add(term_clean)
    
    final_query = " ".join(unique_terms)
    
    # === BÆ¯á»šC 6: Xá»¬ LÃ NGá»® Cáº¢NH VÃ€ Cá»¤M Tá»ª (Cl5Sonnet) ===
    
    # Kiá»ƒm tra cá»¥m tá»« dÃ i trong SYNONYM_MAP
    context_expansions = []
    for phrase, expansion in SYNONYM_MAP.items():
        # Kiá»ƒm tra cá»¥m tá»« trong truy váº¥n gá»‘c
        if phrase in clean_query:
            context_expansions.append(expansion)
        # Kiá»ƒm tra cá»¥m tá»« viáº¿t liá»n
        elif phrase in no_space_query:
            context_expansions.append(expansion)
        # Kiá»ƒm tra cá»¥m tá»« trong truy váº¥n Ä‘Ã£ má»Ÿ rá»™ng
        elif phrase in final_query:
            context_expansions.append(expansion)
    
    # ThÃªm cÃ¡c má»Ÿ rá»™ng ngá»¯ cáº£nh
    for expansion in context_expansions:
        if expansion not in final_query:
            final_query += " " + expansion
    
    # === BÆ¯á»šC 7: LÃ€MM Sáº CH Káº¾T QUáº¢ CUá»I CÃ™NG ===
    final_query = " ".join(final_query.split())  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    
    # Ghi log káº¿t quáº£ xá»­ lÃ½
    logging.info(f"ğŸ”„ TÃCH Há»¢P - Preprocessing query:")
    logging.info(f"   Original: {original_query}")
    logging.info(f"   Clean: {clean_query}")
    logging.info(f"   No space: {no_space_query}")
    logging.info(f"   Context found: {context_found}")
    logging.info(f"   Final: {final_query}")
    
    return final_query if final_query.strip() else clean_query

# Táº¡o snippet thÃ´ng minh - TÃCH Há»¢P GocL4 + Cl5Sonnet
def create_snippet(content, query_clean, max_length=300):
    """
    ğŸ”§ TÃCH Há»¢P HOÃ€N CHá»ˆNH: Táº¡o snippet thÃ´ng minh tá»« GocL4 + Cl5Sonnet
    - Sá»­ dá»¥ng extract_keywords tá»« GocL4
    - TÃ­nh Ä‘iá»ƒm thÃ´ng minh tá»« Cl5Sonnet
    - Táº¡o ngá»¯ cáº£nh xung quanh tá»« khÃ³a
    - Xá»­ lÃ½ nhiá»u loáº¡i ná»™i dung khÃ¡c nhau
    """
    if not content or not query_clean:
        return content[:max_length] + "..." if len(content) > max_length else content
    
    # TÃ¡ch ná»™i dung thÃ nh cÃ¡c cÃ¢u
    sentences = []
    # TÃ¡ch theo dáº¥u cháº¥m, cháº¥m há»i, cháº¥m than, xuá»‘ng dÃ²ng
    raw_sentences = re.split(r'[.!?\n]+', content)
    
    for sentence in raw_sentences:
        sentence = sentence.strip()
        if len(sentence) > 10:  # Bá» qua cÃ¢u quÃ¡ ngáº¯n
            sentences.append(sentence)
    
    if not sentences:
        return content[:max_length] + "..." if len(content) > max_length else content
    
    # ğŸ†• TÃCH Há»¢P GocL4: Sá»­ dá»¥ng extract_keywords Ä‘á»ƒ láº¥y tá»« khÃ³a quan trá»ng
    query_keywords = extract_keywords(query_clean)
    
    if not query_keywords:
        # Náº¿u khÃ´ng cÃ³ tá»« khÃ³a, láº¥y Ä‘oáº¡n Ä‘áº§u
        return content[:max_length] + ("..." if len(content) > max_length else "")
    
    # TÃ­nh Ä‘iá»ƒm cho tá»«ng cÃ¢u dá»±a trÃªn sá»‘ tá»« khÃ³a khá»›p
    sentence_scores = []
    
    for i, sentence in enumerate(sentences):
        sentence_clean = normalize_text(sentence)
        
        # Äáº¿m sá»‘ tá»« khÃ³a khá»›p
        keyword_matches = 0
        phrase_matches = 0
        
        # Kiá»ƒm tra tá»« khÃ³a Ä‘Æ¡n
        for keyword in query_keywords:
            if keyword in sentence_clean:
                keyword_matches += 1
        
        # Kiá»ƒm tra cá»¥m tá»« (2-3 tá»« liÃªn tiáº¿p)
        for j in range(len(query_keywords) - 1):
            phrase = " ".join(query_keywords[j:j+2])
            if phrase in sentence_clean:
                phrase_matches += 2  # Cá»¥m tá»« cÃ³ Ä‘iá»ƒm cao hÆ¡n
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng
        total_score = keyword_matches + phrase_matches
        
        # Æ¯u tiÃªn cÃ¢u á»Ÿ Ä‘áº§u (cÃ¢u tiÃªu Ä‘á» thÆ°á»ng quan trá»ng)
        position_bonus = max(0, 3 - i) * 0.1
        
        final_score = total_score + position_bonus
        
        if final_score > 0:
            sentence_scores.append((sentence, final_score, i))
    
    # Sáº¯p xáº¿p theo Ä‘iá»ƒm sá»‘
    sentence_scores.sort(key=lambda x: x[1], reverse=True)
    
    if not sentence_scores:
        return content[:max_length] + ("..." if len(content) > max_length else "")
    
    # Táº¡o snippet tá»« cÃ¡c cÃ¢u tá»‘t nháº¥t
    snippet_parts = []
    current_length = 0
    used_positions = set()
    
    # Láº¥y cÃ¢u cÃ³ Ä‘iá»ƒm cao nháº¥t trÆ°á»›c
    for sentence, score, position in sentence_scores:
        if current_length + len(sentence) <= max_length:
            snippet_parts.append((sentence, position))
            used_positions.add(position)
            current_length += len(sentence) + 2  # +2 cho khoáº£ng cÃ¡ch
        
        if current_length >= max_length * 0.8:  # Äáº¡t 80% thÃ¬ dá»«ng
            break
    
    # Sáº¯p xáº¿p láº¡i theo thá»© tá»± xuáº¥t hiá»‡n trong vÄƒn báº£n gá»‘c
    snippet_parts.sort(key=lambda x: x[1])
    
    # Táº¡o snippet cuá»‘i cÃ¹ng
    if snippet_parts:
        final_snippet = " ".join([part[0] for part in snippet_parts])
        
        # ThÃªm dáº¥u "..." náº¿u cáº§n
        if current_length >= max_length or len(snippet_parts) < len(sentence_scores):
            final_snippet += "..."
        
        return final_snippet
    
    # Fallback: láº¥y Ä‘oáº¡n Ä‘áº§u
    return content[:max_length] + ("..." if len(content) > max_length else "")

# Chuáº©n hÃ³a + xá»­ lÃ½ viáº¿t liá»n + dá»‹ch viáº¿t táº¯t/tiáº¿ng Anh - TÃCH Há»¢P
def normalize_and_expand(text):
    """
    ğŸ”§ TÃCH Há»¢P GocL4 + Cl5Sonnet: Chuáº©n hÃ³a vÃ  má»Ÿ rá»™ng vÄƒn báº£n thÃ´ng minh
    - Xá»­ lÃ½ viáº¿t liá»n tá»« GocL4
    - Ãp dá»¥ng mapping Ä‘á»“ng nghÄ©a má»Ÿ rá»™ng
    - Giá»¯ nguyÃªn Ã½ nghÄ©a gá»‘c náº¿u khÃ´ng tÃ¬m tháº¥y mapping
    """
    if not text:
        return ""
    
    # Chuáº©n hÃ³a cÆ¡ báº£n
    text_clean = normalize_text(text)
    text_no_space = text_clean.replace(" ", "")
    
    # Kiá»ƒm tra mapping tá»« SYNONYM_MAP má»Ÿ rá»™ng
    if text_no_space in SYNONYM_MAP:
        return SYNONYM_MAP[text_no_space]
    elif text_clean in SYNONYM_MAP:
        return SYNONYM_MAP[text_clean]
    
    # Xá»­ lÃ½ tá»«ng tá»«
    words = text_clean.split()
    expanded_words = []
    
    for word in words:
        if word in SYNONYM_MAP:
            expanded_words.extend(SYNONYM_MAP[word].split())
        else:
            expanded_words.append(word)
    
    return " ".join(expanded_words) if expanded_words else text_clean

# ğŸ†• TÃCH Há»¢P GocL4: TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng chuá»—i (fuzzy matching)
def calculate_string_similarity(str1, str2, method='best'):
    """
    TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 chuá»—i vá»›i nhiá»u phÆ°Æ¡ng phÃ¡p
    method: 'difflib', 'best' (chá»n káº¿t quáº£ tá»‘t nháº¥t)
    """
    if not str1 or not str2:
        return 0.0
    
    str1_clean = normalize_text(str1)
    str2_clean = normalize_text(str2)
    
    if str1_clean == str2_clean:
        return 1.0
    
    similarities = []
    
    # PhÆ°Æ¡ng phÃ¡p 1: difflib SequenceMatcher
    try:
        difflib_sim = difflib.SequenceMatcher(None, str1_clean, str2_clean).ratio()
        similarities.append(difflib_sim)
    except:
        pass
    
    # PhÆ°Æ¡ng phÃ¡p 2: Word-level similarity
    words1 = set(str1_clean.split())
    words2 = set(str2_clean.split())
    
    if words1 or words2:
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        word_sim = intersection / union if union > 0 else 0.0
        similarities.append(word_sim)
    
    # Tráº£ vá» káº¿t quáº£ tÃ¹y theo method
    if not similarities:
        return 0.0
    
    if method == 'best':
        return max(similarities)
    elif method == 'average':
        return sum(similarities) / len(similarities)
    else:
        return similarities[0] if similarities else 0.0

#Ham bá»
# HÃ m kiá»ƒm tra xem tÃªn file Ä‘Ã£ tá»“n táº¡i trong cÆ¡ sá»Ÿ dá»¯ liá»‡u chÆ°a
# def is_file_exists(title):  
#     cursor.execute("SELECT COUNT(*) FROM forms WHERE title = %s", (title,))
#     count = cursor.fetchone()[0]
#     return count > 0

# TrÃ­ch xuáº¥t tiÃªu Ä‘á» tá»« ná»™i dung file - GIá»® NGUYÃŠN Cl5Sonnet
def extract_title_from_content(content):
    """
    ğŸš€ GIá»® NGUYÃŠN Cl5Sonnet: TrÃ­ch xuáº¥t tiÃªu Ä‘á» thÃ´ng minh tá»« ná»™i dung
    - PhÃ¢n tÃ­ch cáº¥u trÃºc vÄƒn báº£n
    - Nháº­n diá»‡n tiÃªu Ä‘á» dá»±a trÃªn pattern
    - Xá»­ lÃ½ nhiá»u Ä‘á»‹nh dáº¡ng khÃ¡c nhau
    """
    if not content:
        return ""
    
    lines = content.split('\n')
    potential_titles = []
    
    # Duyá»‡t qua 10 dÃ²ng Ä‘áº§u tiÃªn
    for i, line in enumerate(lines[:10]):
        line = line.strip()
        
        # Bá» qua dÃ²ng quÃ¡ ngáº¯n hoáº·c quÃ¡ dÃ i
        if len(line) < 5 or len(line) > 150:
            continue
        
        # TiÃªu Ä‘á» thÆ°á»ng cÃ³ cÃ¡c Ä‘áº·c Ä‘iá»ƒm:
        score = 0
        line_lower = normalize_text(line)
        
        # 1. Chá»©a tá»« khÃ³a Ä‘áº·c trÆ°ng cá»§a biá»ƒu máº«u
        title_keywords = [
            'Ä‘Æ¡n', 'biá»ƒu máº«u', 'giáº¥y', 'thÃ´ng bÃ¡o', 'quyáº¿t Ä‘á»‹nh', 
            'bÃ¡o cÃ¡o', 'tá» khai', 'phiáº¿u', 'báº£n kÃª', 'danh sÃ¡ch',
            'xin', 'Ä‘á» nghá»‹', 'kiáº¿n nghá»‹', 'tham kháº£o'
        ]
        
        for keyword in title_keywords:
            if keyword in line_lower:
                score += 2
        
        # 2. Vá»‹ trÃ­ trong vÄƒn báº£n (cÃ ng Ä‘áº§u cÃ ng tá»‘t)
        if i == 0:
            score += 3
        elif i <= 2:
            score += 2
        elif i <= 5:
            score += 1
        
        # 3. Äá»‹nh dáº¡ng (viáº¿t hoa, khÃ´ng cÃ³ dáº¥u cháº¥m cÃ¢u nhiá»u)
        if line.isupper() or line.istitle():
            score += 1
        
        if not line.endswith('.'):
            score += 1
        
        # 4. KhÃ´ng chá»©a quÃ¡ nhiá»u sá»‘
        digit_count = sum(1 for c in line if c.isdigit())
        if digit_count / len(line) < 0.3:  # Ãt hÆ¡n 30% lÃ  sá»‘
            score += 1
        
        potential_titles.append((score, line, i))
    
    # Sáº¯p xáº¿p theo Ä‘iá»ƒm sá»‘ vÃ  chá»n tiÃªu Ä‘á» tá»‘t nháº¥t
    potential_titles.sort(key=lambda x: x[0], reverse=True)
    
    if potential_titles and potential_titles[0][0] > 2:  # Äiá»ƒm tá»‘i thiá»ƒu
        return potential_titles[0][1]
    
    return ""

# ğŸ†• TÃCH Há»¢P: TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a truy váº¥n vÃ  tÃªn file - Cl5Sonnet + GocL4
def calculate_filename_similarity(query_clean, filename_clean):
    """
    ğŸš€ TÃCH Há»¢P: TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng thÃ´ng minh giá»¯a truy váº¥n vÃ  tÃªn file
    - Sá»­ dá»¥ng calculate_string_similarity tá»« GocL4
    - Káº¿t há»£p vá»›i logic Cl5Sonnet
    - Æ¯u tiÃªn khá»›p chÃ­nh xÃ¡c vÃ  khá»›p tá»« khÃ³a
    """
    if not query_clean or not filename_clean:
        return 0.0
    
    # PhÆ°Æ¡ng phÃ¡p 1: Sá»­ dá»¥ng calculate_string_similarity tá»« GocL4
    string_sim = calculate_string_similarity(query_clean, filename_clean)
    
    # PhÆ°Æ¡ng phÃ¡p 2: Kiá»ƒm tra chá»©a tá»« khÃ³a (Cl5Sonnet)
    query_words = set(query_clean.split())
    filename_words = set(filename_clean.split())
    
    if not query_words:
        return string_sim
    
    # TÃ­nh tá»· lá»‡ tá»« khÃ³a chung
    common_words = query_words.intersection(filename_words)
    keyword_ratio = len(common_words) / len(query_words) if query_words else 0
    
    # PhÆ°Æ¡ng phÃ¡p 3: Kiá»ƒm tra khá»›p substring (Cl5Sonnet)
    substring_bonus = 0
    if query_clean in filename_clean:
        substring_bonus = 0.3
    elif any(word in filename_clean for word in query_words if len(word) > 3):
        substring_bonus = 0.15
    
    # TÃ­nh Ä‘iá»ƒm tá»•ng há»£p
    final_score = max(string_sim, keyword_ratio) + substring_bonus
    
    # Äáº£m báº£o khÃ´ng vÆ°á»£t quÃ¡ 1.0
    return min(1.0, final_score)

# TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a truy váº¥n vÃ  tiÃªu Ä‘á» - GIá»® NGUYÃŠN Cl5Sonnet + Cáº¢I TIáº¾N
def calculate_title_similarity(query_clean, title_clean):
    """
    ğŸš€ NÃ‚NG Cáº¤P Cl5Sonnet: TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng thÃ´ng minh giá»¯a truy váº¥n vÃ  tiÃªu Ä‘á»
    - Æ¯u tiÃªn khá»›p tá»« khÃ³a quan trá»ng
    - Xá»­ lÃ½ tá»« Ä‘á»“ng nghÄ©a vÃ  viáº¿t táº¯t
    - TÃ­nh toÃ¡n ngá»¯ cáº£nh
    - TÃ­ch há»£p vá»›i calculate_string_similarity tá»« GocL4
    """
    if not query_clean or not title_clean:
        return 0.0
    
    query_words = set(query_clean.split())
    title_words = set(title_clean.split())
    
    if not query_words:
        return 0.0
    
    # TÃ­nh tá»· lá»‡ tá»« khÃ³a chung
    common_words = query_words.intersection(title_words)
    basic_similarity = len(common_words) / len(query_words)
    
    # ğŸ†• TÃCH Há»¢P GocL4: Sá»­ dá»¥ng calculate_string_similarity
    string_similarity = calculate_string_similarity(query_clean, title_clean)
    
    # Bonus cho khá»›p substring
    substring_bonus = 0
    if query_clean in title_clean:
        substring_bonus = 0.4
    elif any(word in title_clean for word in query_words if len(word) > 3):
        substring_bonus = 0.2
    
    # Bonus cho tá»« khÃ³a quan trá»ng (tá»« dÃ i)
    important_word_bonus = 0
    for word in query_words:
        if len(word) > 4 and word in title_clean:
            important_word_bonus += 0.1
    
    # TÃ­nh Ä‘iá»ƒm cuá»‘i cÃ¹ng - káº¿t há»£p nhiá»u phÆ°Æ¡ng phÃ¡p
    final_score = max(basic_similarity, string_similarity) + substring_bonus + important_word_bonus
    
    return min(1.0, final_score)

# TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a truy váº¥n vÃ  ná»™i dung - TÃCH Há»¢P GocL4 + Cl5Sonnet
def calculate_content_similarity(query_clean, content_clean):
    """
    ğŸš€ TÃCH Há»¢P: TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng thÃ´ng minh giá»¯a truy váº¥n vÃ  ná»™i dung
    - Sá»­ dá»¥ng extract_keywords tá»« GocL4
    - TÃ­nh táº§n suáº¥t tá»« khÃ³a tá»« Cl5Sonnet
    - Xá»­ lÃ½ Ä‘á»™ dÃ i ná»™i dung
    - Æ¯u tiÃªn tá»« khÃ³a quan trá»ng
    """
    if not query_clean or not content_clean:
        return 0.0
    
    # ğŸ†• TÃCH Há»¢P GocL4: Sá»­ dá»¥ng extract_keywords Ä‘á»ƒ láº¥y tá»« khÃ³a quan trá»ng
    query_keywords = extract_keywords(query_clean)
    
    if not query_keywords:
        return 0.0
    
    # Äáº¿m tá»« khÃ³a trong ná»™i dung
    total_matches = 0
    unique_matches = 0
    
    for word in query_keywords:
        if word in content_clean:
            count = content_clean.count(word)
            total_matches += count
            unique_matches += 1
    
    # TÃ­nh Ä‘iá»ƒm dá»±a trÃªn tá»· lá»‡ tá»« khÃ³a unique
    unique_ratio = unique_matches / len(query_keywords)
    
    # TÃ­nh Ä‘iá»ƒm dá»±a trÃªn táº§n suáº¥t xuáº¥t hiá»‡n
    frequency_score = min(total_matches / len(query_keywords), 2.0) / 2.0  # Chuáº©n hÃ³a vá» [0,1]
    
    # Káº¿t há»£p hai Ä‘iá»ƒm sá»‘
    final_score = (unique_ratio * 0.7) + (frequency_score * 0.3)
    
    return min(1.0, final_score)


# ===== API SEARCH TÃCH Há»¢P HOÃ€N CHá»ˆNH =====
"""
âœ… PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m thÃ´ng minh tá»« GocL4
âœ… Chuáº©n hÃ³a truy váº¥n tiáº¿ng Viá»‡t toÃ n diá»‡n tá»« GocL4
âœ… Xá»­ lÃ½ viáº¿t táº¯t, khÃ´ng dáº¥u, viáº¿t liá»n vá»›i SYNONYM_MAP má»Ÿ rá»™ng
âœ… Hiá»ƒu ngá»¯ cáº£nh cáº£m xÃºc vá»›i CONTEXT_INTENT_MAP tá»« GocL4
âœ… TÃ¬m kiáº¿m real-time vá»›i should_return_results tá»« GocL4
âœ… Snippet thÃ´ng minh vá»›i extract_keywords tá»« GocL4
âœ… TÃ­nh Ä‘iá»ƒm Ä‘a táº§ng vá»›i trá»ng sá»‘ Ä‘á»™ng tá»« Cl5Sonnet
âœ… Xá»­ lÃ½ trÆ°á»ng há»£p Ä‘áº·c biá»‡t (di = Ä‘iá»ƒm/Ä‘i) tá»« GocL4
âœ… Logging chi tiáº¿t vÃ  thá»‘ng kÃª hiá»‡u suáº¥t tá»« GocL4
âœ… Táº¥t cáº£ tÃ­nh nÄƒng máº¡nh máº½ cá»§a Cl5Sonnet Ä‘Æ°á»£c giá»¯ nguyÃªn
"""
    
@app.get("/search")
def semantic_search(
    query: str = Query(..., description="CÃ¢u truy váº¥n tÃ¬m kiáº¿m"),
    top_k: int = Query(10, description="Sá»‘ lÆ°á»£ng káº¿t quáº£ tá»‘i Ä‘a"),
    min_score: float = Query(0.15, description="Äiá»ƒm tÆ°Æ¡ng Ä‘á»“ng tá»‘i thiá»ƒu")
):
    # ğŸ†• TÃCH Há»¢P GocL4: Ghi log báº¯t Ä‘áº§u tÃ¬m kiáº¿m vá»›i thÃ´ng tin chi tiáº¿t
    start_time = time.time()
    logging.info(f"ğŸš€ === Báº®T Äáº¦U TÃŒM KIáº¾M TÃCH Há»¢P HOÃ€N CHá»ˆNH V4.0 ===")
    logging.info(f"Query gá»‘c: '{query}'")
    logging.info(f"Tham sá»‘: top_k={top_k}, min_score={min_score}")
    
    try:
        # ===== BÆ¯á»šC 1: KIá»‚M TRA VÃ€ CHUáº¨N Bá»Š QUERY (TÃCH Há»¢P GocL4) =====
        original_query = query.strip()
        
        # ğŸ†• TÃCH Há»¢P GocL4: Kiá»ƒm tra query cÃ³ há»£p lá»‡ khÃ´ng (cho real-time search)
        should_search, reason = should_return_results(original_query)
        if not should_search:
            return {
                "query": original_query,
                "results": [],
                "search_type": reason,
                "message": "Query quÃ¡ ngáº¯n hoáº·c khÃ´ng há»£p lá»‡",
                "processing_time": 0.0
            }
        
        # ===== BÆ¯á»šC 2: PHÃ‚N TÃCH VÃ€ TIá»€N Xá»¬ LÃ QUERY (TÃCH Há»¢P HOÃ€N CHá»ˆNH) =====
        
        # ğŸ†• TÃCH Há»¢P GocL4: PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m
        search_intent = analyze_search_intent(original_query)
        logging.info(f"ğŸ¯ Ã Ä‘á»‹nh tÃ¬m kiáº¿m: {search_intent}")
        
        # Chuáº©n bá»‹ cÃ¡c biáº¿n thá»ƒ cá»§a query vá»›i preprocessing tÃ­ch há»£p hoÃ n chá»‰nh
        query_clean = normalize_text(original_query)
        query_no_space = query_clean.replace(" ", "")
        query_expanded = preprocess_query(original_query)  # ğŸ†• Sá»­ dá»¥ng preprocess_query tÃ­ch há»£p
        query_keywords = extract_keywords(query_expanded)  # ğŸ†• Sá»­ dá»¥ng extract_keywords tá»« GocL4
        
        logging.info(f"ğŸ“ Query sau xá»­ lÃ½ tÃ­ch há»£p hoÃ n chá»‰nh:")
        logging.info(f"   - Clean: '{query_clean}'")
        logging.info(f"   - No space: '{query_no_space}'")
        logging.info(f"   - Expanded: '{query_expanded}'")
        logging.info(f"   - Keywords: {query_keywords}")
        
        # PhÃ¢n loáº¡i Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng (GIá»® NGUYÃŠN logic Cl5Sonnet + bá»• sung GocL4)
        is_exact_filename = (
            query_clean.endswith('.docx') or 
            query_clean.endswith('.pdf') or
            len(query_no_space) > 20 or  # TÃªn file thÆ°á»ng ráº¥t dÃ i
            any(ext in query_clean for ext in ['.doc', '.txt', '.xlsx'])
        ) or search_intent == 'filename'  # ğŸ†• TÃCH Há»¢P: ThÃªm Ä‘iá»u kiá»‡n tá»« analyze_search_intent
        
        # ğŸ†• TÃCH Há»¢P GocL4: XÃ¡c Ä‘á»‹nh xem cÃ³ pháº£i tÃ¬m kiáº¿m theo ngá»¯ cáº£nh khÃ´ng
        is_contextual_search = any(keyword in query_expanded for keyword in [
            'muá»‘n', 'cáº§n', 'khÃ´ng', 'thÃ­ch', 'mong muá»‘n', 'hy vá»ng'
        ]) or search_intent == 'semantic'  # ğŸ†• TÃCH Há»¢P: ThÃªm Ä‘iá»u kiá»‡n tá»« analyze_search_intent
        
        logging.info(f"ğŸ¯ Search intent analysis tÃ­ch há»£p:")
        logging.info(f"   - Search type: {search_intent}")
        logging.info(f"   - Exact filename: {is_exact_filename}")
        logging.info(f"   - Contextual: {is_contextual_search}")
        
        # ===== BÆ¯á»šC 3: Táº O VECTOR TRUY Váº¤N ÄA CHIáº¾N LÆ¯á»¢C (GIá»® NGUYÃŠN Cl5Sonnet) =====
        query_vectors = {}
        
        try:
            # Vector cho truy váº¥n gá»‘c
            query_vectors['original'] = model.encode(original_query).tolist()
            
            # Vector cho truy váº¥n Ä‘Ã£ chuáº©n hÃ³a
            if query_clean != original_query:
                query_vectors['clean'] = model.encode(query_clean).tolist()
            
            # Vector cho truy váº¥n Ä‘Ã£ má»Ÿ rá»™ng (tÃ­ch há»£p preprocess_query tá»« GocL4)
            if query_expanded != query_clean:
                query_vectors['expanded'] = model.encode(query_expanded).tolist()
                
            logging.info(f"âœ… Created {len(query_vectors)} query vectors")
            
        except Exception as e:
            logging.error(f"âŒ Lá»—i táº¡o vector: {e}")
            return JSONResponse(
                status_code=500,
                content={"error": "KhÃ´ng thá»ƒ xá»­ lÃ½ truy váº¥n", "detail": str(e)}
            )
        
        # ===== BÆ¯á»šC 4: Láº¤Y Dá»® LIá»†U Tá»ª DATABASE =====
        cursor.execute("SELECT id, title, content, embedding FROM forms")
        rows = cursor.fetchall()
        
        if not rows:
            processing_time = time.time() - start_time  # ğŸ†• TÃCH Há»¢P GocL4
            logging.info("ğŸ“­ KhÃ´ng cÃ³ dá»¯ liá»‡u trong database")
            return {
                "query": original_query,
                "results": [],
                "search_info": {
                    "total_documents": 0,
                    "total_found": 0,
                    "returned": 0,
                    "search_intent": search_intent,  # ğŸ†• TÃCH Há»¢P GocL4
                    "processing_time": round(processing_time, 3)
                }
            }
        
        logging.info(f"ğŸ“Š Processing {len(rows)} documents from database")
        
        # ===== BÆ¯á»šC 5: Xá»¬ LÃ Tá»ªNG BIá»‚U MáºªU (TÃCH Há»¢P HOÃ€N CHá»ˆNH) =====
        results = []
        
        for doc_id, title, content, embedding in rows:
            try:
                # Chuáº©n hÃ³a dá»¯ liá»‡u biá»ƒu máº«u
                title_clean = normalize_text(title)
                title_no_space = title_clean.replace(" ", "")
                content_clean = normalize_text(content)
                
                # ğŸ†• TÃCH Há»¢P Cl5Sonnet: TrÃ­ch xuáº¥t tiÃªu Ä‘á» tá»« ná»™i dung
                extracted_title = extract_title_from_content(content)
                extracted_title_clean = normalize_text(extracted_title) if extracted_title else ""
                
                # === CHIáº¾N LÆ¯á»¢C 1: KHá»šP TÃŠN FILE (TÃCH Há»¢P GocL4 + Cl5Sonnet) ===
                filename_score = 0.0
                filename_reason = ""
                
                if is_exact_filename or search_intent == 'filename':
                    # ğŸ†• TÃCH Há»¢P: Sá»­ dá»¥ng calculate_filename_similarity tÃ­ch há»£p
                    filename_score = calculate_filename_similarity(query_clean, title_clean)
                    
                    # Khá»›p hoÃ n toÃ n
                    if query_no_space == title_no_space:
                        filename_score = 1.0
                        filename_reason = "ğŸ¯ Khá»›p chÃ­nh xÃ¡c tÃªn file"
                    # Khá»›p má»™t pháº§n lá»›n
                    elif query_no_space in title_no_space:
                        filename_score = max(filename_score, 0.95)
                        filename_reason = "ğŸ¯ Chá»©a tÃªn file"
                    elif title_no_space in query_no_space:
                        filename_score = max(filename_score, 0.9)
                        filename_reason = "ğŸ¯ TÃªn file náº±m trong truy váº¥n"
                    elif filename_score > 0.7:
                        filename_reason = "ğŸ“„ TÃªn file tÆ°Æ¡ng Ä‘á»“ng cao"
                else:
                    # Vá»›i truy váº¥n thÃ´ng thÆ°á»ng, chá»‰ xÃ©t náº¿u cÃ³ khá»›p Ä‘Ã¡ng ká»ƒ
                    if query_clean in title_clean or any(word in title_clean for word in query_clean.split() if len(word) > 3):
                        filename_score = calculate_filename_similarity(query_clean, title_clean)
                        if filename_score > 0.3:
                            filename_reason = "ğŸ“„ TÃªn file cÃ³ tá»« khÃ³a"
                
                # === CHIáº¾N LÆ¯á»¢C 2: KHá»šP TIÃŠU Äá»€ (TÃCH Há»¢P GocL4 + Cl5Sonnet) ===
                title_score = 0.0
                title_reason = ""
                
                # Kiá»ƒm tra vá»›i tiÃªu Ä‘á» tá»« tÃªn file
                title_scores = []
                
                # ğŸ†• TÃCH Há»¢P: Sá»­ dá»¥ng calculate_title_similarity tÃ­ch há»£p
                # So sÃ¡nh vá»›i tiÃªu Ä‘á» gá»‘c
                if query_clean in title_clean:
                    title_scores.append((0.9, "ğŸ“‹ TiÃªu Ä‘á» chá»©a truy váº¥n chÃ­nh xÃ¡c"))
                elif query_expanded in title_clean:
                    title_scores.append((0.85, "ğŸ“‹ TiÃªu Ä‘á» chá»©a tá»« má»Ÿ rá»™ng"))
                else:
                    basic_title_score = calculate_title_similarity(query_clean, title_clean)
                    if basic_title_score > 0.3:
                        title_scores.append((basic_title_score, "ğŸ“ TiÃªu Ä‘á» tÆ°Æ¡ng Ä‘á»“ng"))
                
                # So sÃ¡nh vá»›i tiÃªu Ä‘á» trÃ­ch xuáº¥t tá»« ná»™i dung
                if extracted_title_clean:
                    if query_clean in extracted_title_clean:
                        title_scores.append((0.95, "ğŸ“Š TiÃªu Ä‘á» ná»™i dung khá»›p chÃ­nh xÃ¡c"))
                    elif query_expanded in extracted_title_clean:
                        title_scores.append((0.9, "ğŸ“Š TiÃªu Ä‘á» ná»™i dung chá»©a tá»« má»Ÿ rá»™ng"))
                    else:
                        extracted_score = calculate_title_similarity(query_clean, extracted_title_clean)
                        if extracted_score > 0.3:
                            title_scores.append((extracted_score, "ğŸ“Š TiÃªu Ä‘á» ná»™i dung tÆ°Æ¡ng Ä‘á»“ng"))
                
                # Láº¥y Ä‘iá»ƒm tiÃªu Ä‘á» cao nháº¥t
                if title_scores:
                    title_score, title_reason = max(title_scores, key=lambda x: x[0])
                
                # === CHIáº¾N LÆ¯á»¢C 3: KHá»šP Ná»˜I DUNG (TÃCH Há»¢P GocL4 + Cl5Sonnet) ===
                content_score = 0.0
                content_reason = ""
                
                # ğŸ†• TÃCH Há»¢P: Sá»­ dá»¥ng calculate_content_similarity tÃ­ch há»£p vá»›i extract_keywords
                basic_content_score = calculate_content_similarity(query_clean, content_clean)
                
                # TÃ­nh Ä‘iá»ƒm vá»›i truy váº¥n má»Ÿ rá»™ng
                expanded_content_score = 0.0
                if query_expanded != query_clean:
                    expanded_content_score = calculate_content_similarity(query_expanded, content_clean)
                
                # Láº¥y Ä‘iá»ƒm cao nháº¥t
                content_score = max(basic_content_score, expanded_content_score)
                
                if content_score > 0.6:
                    content_reason = "ğŸ“š Ná»™i dung chá»©a nhiá»u tá»« khÃ³a quan trá»ng"
                elif content_score > 0.4:
                    content_reason = "ğŸ“– Ná»™i dung cÃ³ tá»« khÃ³a liÃªn quan"  
                elif content_score > 0.2:
                    content_reason = "ğŸ“„ Ná»™i dung cÃ³ Ã­t tá»« khÃ³a"
                
                # === CHIáº¾N LÆ¯á»¢C 4: SEMANTIC SIMILARITY (GIá»® NGUYÃŠN Cl5Sonnet) ===
                semantic_scores = []
                
                try:
                    for vec_type, query_vec in query_vectors.items():
                        sem_score = cosine_similarity(query_vec, embedding)
                        semantic_scores.append(sem_score)
                        
                    max_semantic_score = max(semantic_scores) if semantic_scores else 0.0
                    avg_semantic_score = sum(semantic_scores) / len(semantic_scores) if semantic_scores else 0.0
                    
                    # Sá»­ dá»¥ng Ä‘iá»ƒm trung bÃ¬nh Ä‘á»ƒ á»•n Ä‘á»‹nh hÆ¡n
                    final_semantic_score = (max_semantic_score + avg_semantic_score) / 2
                    
                except Exception as e:
                    logging.error(f"Lá»—i tÃ­nh semantic similarity: {e}")
                    final_semantic_score = 0.0
                
                semantic_reason = ""
                if final_semantic_score > 0.8:
                    semantic_reason = "ğŸ¤– TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a ráº¥t cao"
                elif final_semantic_score > 0.6:
                    semantic_reason = "ğŸ¤– TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a cao"
                elif final_semantic_score > 0.4:
                    semantic_reason = "ğŸ¤– TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a vá»«a"
                elif final_semantic_score > 0.2:
                    semantic_reason = "ğŸ¤– TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a tháº¥p"
                
                # ===== BÆ¯á»šC 6: TÃNH ÄIá»‚M Tá»”NG Há»¢P THÃ”NG MINH (TÃCH Há»¢P HOÃ€N CHá»ˆNH) =====
                
                # ğŸ†• TÃCH Há»¢P GocL4: XÃ¡c Ä‘á»‹nh trá»ng sá»‘ dá»±a trÃªn Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m tá»« analyze_search_intent
                if search_intent == 'filename' or is_exact_filename:
                    # TÃ¬m file cá»¥ thá»ƒ: Æ°u tiÃªn tÃªn file
                    weights = {
                        'filename': 0.7,
                        'title': 0.15, 
                        'content': 0.1,
                        'semantic': 0.05
                    }
                elif search_intent == 'semantic' or is_contextual_search:
                    # TÃ¬m theo ngá»¯ cáº£nh: Æ°u tiÃªn semantic vÃ  ná»™i dung
                    weights = {
                        'filename': 0.05,
                        'title': 0.25,
                        'content': 0.35, 
                        'semantic': 0.35
                    }
                elif search_intent == 'title':
                    # TÃ¬m theo tiÃªu Ä‘á»: Æ°u tiÃªn tiÃªu Ä‘á»
                    weights = {
                        'filename': 0.1,
                        'title': 0.6,
                        'content': 0.2,
                        'semantic': 0.1
                    }
                elif search_intent == 'content':
                    # TÃ¬m theo ná»™i dung: Æ°u tiÃªn ná»™i dung
                    weights = {
                        'filename': 0.05,
                        'title': 0.2,
                        'content': 0.5,
                        'semantic': 0.25
                    }
                else:
                    # TÃ¬m kiáº¿m tá»•ng há»£p: cÃ¢n báº±ng (GIá»® NGUYÃŠN Cl5Sonnet)
                    weights = {
                        'filename': 0.15,
                        'title': 0.35,
                        'content': 0.3,
                        'semantic': 0.2
                    }
                
                # TÃ­nh Ä‘iá»ƒm tá»•ng há»£p
                composite_score = (
                    filename_score * weights['filename'] +
                    title_score * weights['title'] +
                    content_score * weights['content'] +
                    final_semantic_score * weights['semantic']
                )
                
                # === BONUS ÄIá»‚M (TÃCH Há»¢P GocL4 + Cl5Sonnet) ===
                
                # ğŸ†• TÃCH Há»¢P: Bonus cho káº¿t quáº£ cÃ³ nhiá»u chiáº¿n lÆ°á»£c thÃ nh cÃ´ng
                successful_strategies = sum([
                    1 for score in [filename_score, title_score, content_score, final_semantic_score] 
                    if score > 0.4
                ])
                
                strategy_bonus = 0
                if successful_strategies >= 3:
                    strategy_bonus = 0.15  # Bonus 15% cho 3+ chiáº¿n lÆ°á»£c
                elif successful_strategies >= 2:
                    strategy_bonus = 0.1   # Bonus 10% cho 2+ chiáº¿n lÆ°á»£c
                
                # ğŸ†• TÃCH Há»¢P GocL4: Bonus cho tá»« khÃ³a quan trá»ng (sá»­ dá»¥ng extract_keywords)
                important_keywords = ['Ä‘Æ¡n', 'xin', 'chuyá»ƒn', 'nghá»‰', 'há»c', 'bá»•ng']
                keyword_bonus = 0
                for keyword in important_keywords:
                    if keyword in query_keywords and keyword in (title_clean + " " + content_clean):
                        keyword_bonus += 0.02  # 2% cho má»—i tá»« khÃ³a quan trá»ng
                
                # TÃ­nh Ä‘iá»ƒm cuá»‘i cÃ¹ng
                final_score = composite_score + strategy_bonus + keyword_bonus
                final_score = min(1.0, final_score)  # Äáº£m báº£o khÃ´ng vÆ°á»£t quÃ¡ 1.0
                
                # ===== BÆ¯á»šC 7: Lá»ŒC VÃ€ THÃŠM Káº¾T QUáº¢ =====
                if final_score >= min_score:
                    
                    # XÃ¡c Ä‘á»‹nh lÃ½ do chÃ­nh (chiáº¿n lÆ°á»£c tá»‘t nháº¥t)
                    strategy_scores = [
                        (filename_score, filename_reason),
                        (title_score, title_reason), 
                        (content_score, content_reason),
                        (final_semantic_score, semantic_reason)
                    ]
                    
                    # Láº¥y lÃ½ do cÃ³ Ä‘iá»ƒm cao nháº¥t vÃ  cÃ³ ná»™i dung
                    best_reason = ""
                    for score, reason in sorted(strategy_scores, key=lambda x: x[0], reverse=True):
                        if reason:
                            best_reason = reason
                            break
                    
                    if not best_reason:
                        best_reason = "ğŸ“„ CÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i truy váº¥n"
                    
                    # ğŸ†• TÃCH Há»¢P: Táº¡o snippet thÃ´ng minh vá»›i hÃ m tÃ­ch há»£p tá»« GocL4 + Cl5Sonnet
                    snippet = create_snippet(content, query_expanded, max_length=400)
                    
                    # ThÃ´ng tin debug tÃ­ch há»£p
                    debug_info = {
                        "filename_score": round(filename_score, 3),
                        "title_score": round(title_score, 3), 
                        "content_score": round(content_score, 3),
                        "semantic_score": round(final_semantic_score, 3),
                        "composite_score": round(composite_score, 3),
                        "bonuses": round(strategy_bonus + keyword_bonus, 3),
                        "successful_strategies": successful_strategies,
                        "search_intent": search_intent,  # ğŸ†• TÃCH Há»¢P GocL4
                        "weights_used": weights,  # ğŸ†• TÃCH Há»¢P GocL4
                        "query_keywords": query_keywords  # ğŸ†• TÃCH Há»¢P GocL4
                    }
                    
                    results.append({
                        "id": doc_id,
                        "title": title,
                        "snippet": snippet,
                        "similarity_score": round(final_score, 4),
                        "reason": best_reason,
                        "extracted_title": extracted_title if extracted_title else None,
                        "details": debug_info,
                        "search_intent": search_intent  # ğŸ†• TÃCH Há»¢P GocL4
                    })
                    
                    logging.info(f"âœ… Biá»ƒu máº«u '{title[:30]}...' - Äiá»ƒm: {final_score:.4f} - {best_reason}")
                
            except Exception as e:
                logging.error(f"âŒ Lá»—i xá»­ lÃ½ biá»ƒu máº«u '{title}': {e}")
                continue
        
        # ===== BÆ¯á»šC 8: Sáº®P Xáº¾P VÃ€ Tá»I Æ¯U Káº¾T QUáº¢ =====
        if not results:
            processing_time = time.time() - start_time
            logging.info("ğŸ“­ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p")
            return {
                "query": original_query,
                "results": [],
                "search_info": {
                    "total_documents": len(rows),
                    "total_found": 0,
                    "returned": 0,
                    "search_intent": search_intent,  # ğŸ†• TÃCH Há»¢P GocL4
                    "search_strategies": {
                        "exact_filename": is_exact_filename,
                        "contextual": is_contextual_search,
                        "intent_detected": search_intent
                    },
                    "query_processing": {
                        "original": original_query,
                        "clean": query_clean,
                        "expanded": query_expanded,
                        "keywords": query_keywords  # ğŸ†• TÃCH Há»¢P GocL4
                    },
                    "processing_time": round(processing_time, 3)
                }
            }
        
        # Sáº¯p xáº¿p káº¿t quáº£ theo Ä‘iá»ƒm sá»‘
        sorted_results = sorted(results, key=lambda x: x["similarity_score"], reverse=True)
        
        # ===== BÆ¯á»šC 9: Tá»I Æ¯U Sá» LÆ¯á»¢NG Káº¾T QUáº¢ Tá»° Äá»˜NG (TÃCH Há»¢P GocL4 + Cl5Sonnet) =====
        actual_top_k = top_k
        
        if len(sorted_results) > 1:
            highest_score = sorted_results[0]["similarity_score"]
            
            # ğŸ†• TÃCH Há»¢P GocL4: Case 1: CÃ³ káº¿t quáº£ ráº¥t tá»‘t (>0.9)
            if highest_score > 0.9:
                # Chá»‰ giá»¯ láº¡i káº¿t quáº£ cÃ³ Ä‘iá»ƒm sá»‘ gáº§n vá»›i káº¿t quáº£ tá»‘t nháº¥t
                cutoff_threshold = max(0.6, highest_score * 0.7)
                actual_top_k = 0
                for i, result in enumerate(sorted_results):
                    if result["similarity_score"] >= cutoff_threshold:
                        actual_top_k = i + 1
                    else:
                        break
                actual_top_k = min(actual_top_k, top_k)
                logging.info(f"ğŸ¯ Äiá»u chá»‰nh káº¿t quáº£ do cÃ³ Ä‘iá»ƒm ráº¥t cao: {actual_top_k}")
                
            # Case 2: Káº¿t quáº£ trung bÃ¬nh (0.5-0.9) - GIá»® NGUYÃŠN Cl5Sonnet
            elif highest_score > 0.5:
                # Giá»¯ nguyÃªn top_k nhÆ°ng cÃ³ thá»ƒ cáº¯t bá»›t náº¿u chÃªnh lá»‡ch quÃ¡ lá»›n
                cutoff_threshold = max(0.3, highest_score * 0.5)
                actual_top_k = 0
                for i, result in enumerate(sorted_results[:top_k]):
                    if result["similarity_score"] >= cutoff_threshold:
                        actual_top_k = i + 1
                    else:
                        break
                        
            # Case 3: Káº¿t quáº£ kÃ©m (<0.5) - TÃCH Há»¢P GocL4
            else:
                # Má»Ÿ rá»™ng tÃ¬m kiáº¿m nháº¹
                actual_top_k = min(len(sorted_results), top_k + 2)
                logging.info(f"ğŸ”„ Má»Ÿ rá»™ng káº¿t quáº£ do Ä‘iá»ƒm tháº¥p: {actual_top_k}")
        
        # Cáº¯t káº¿t quáº£ cuá»‘i cÃ¹ng
        final_results = sorted_results[:actual_top_k]
        processing_time = time.time() - start_time
        
        # ===== BÆ¯á»šC 10: LOG VÃ€ TRáº¢ Vá»€ Káº¾T QUáº¢ (TÃCH Há»¢P GocL4) =====
        logging.info(f"ğŸ === Káº¾T THÃšC TÃŒM KIáº¾M TÃCH Há»¢P HOÃ€N CHá»ˆNH V4.0 ===")
        logging.info(f"   - Documents processed: {len(rows)}")
        logging.info(f"   - Results found: {len(results)}")
        logging.info(f"   - Results returned: {len(final_results)}")
        logging.info(f"   - Search intent: {search_intent}")
        logging.info(f"   - Processing time: {processing_time:.3f}s")
        
        if final_results:
            logging.info(f"   - Best score: {final_results[0]['similarity_score']}")
            logging.info(f"   - Worst score: {final_results[-1]['similarity_score']}")
            logging.info(f"ğŸ“Š Top 3 results:")
            for i, result in enumerate(final_results[:3]):
                logging.info(f"     {i+1}. {result['title'][:40]}... - {result['similarity_score']:.4f} - {result['reason']}")
        
        return {
            "query": original_query,
            "search_intent": search_intent,  # ğŸ†• TÃCH Há»¢P GocL4
            "processed_query": query_expanded if query_expanded != query_clean else query_clean,
            "results": final_results,
            "search_info": {
                "total_documents": len(rows),
                "total_found": len(results),
                "returned": len(final_results),
                "search_strategies": {
                    "exact_filename": is_exact_filename,
                    "contextual": is_contextual_search,
                    "intent_detected": search_intent,  # ğŸ†• TÃCH Há»¢P GocL4
                    "weights_used": weights if 'weights' in locals() else None
                },
                "query_processing": {
                    "original": original_query,
                    "clean": query_clean,
                    "expanded": query_expanded if query_expanded != query_clean else None,
                    "keywords": query_keywords,  # ğŸ†• TÃCH Há»¢P GocL4
                    "keyword_count": len(query_keywords)
                },
                "performance": {
                    "processing_time": round(processing_time, 3),
                    "min_score_used": min_score,
                    "auto_adjusted_top_k": actual_top_k != top_k,
                    "best_strategy": final_results[0]["reason"] if final_results else None,
                    "vectors_created": len(query_vectors)
                }
            }
        }
        
    except Exception as e:
        error_time = time.time() - start_time
        logging.error(f"âŒ Lá»–I Há»† THá»NG TÃŒM KIáº¾M TÃCH Há»¢P HOÃ€N CHá»ˆNH V4.0: {e}")
        logging.error(f"Query gÃ¢y lá»—i: '{original_query}'")
        logging.error(f"Thá»i gian trÆ°á»›c lá»—i: {error_time:.3f}s")
        
        return JSONResponse(
            status_code=500,
            content={
                "error": "Lá»—i há»‡ thá»‘ng tÃ¬m kiáº¿m tÃ­ch há»£p hoÃ n chá»‰nh", 
                "detail": str(e),
                "query": original_query if 'original_query' in locals() else query,
                "processing_time": round(error_time, 3)
            }
        )


@app.post("/get-embedding")
def get_embedding(text: Union[str, List[str]] = Body(...)):
    try:
        if isinstance(text, str):
            text = [text]
        text = [t[:5000] for t in text]
        embeddings = model.encode(text).tolist()
        return {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings
        }
    except Exception as e:
        logging.error(f"Lá»—i khi sinh embedding: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"Lá»—i khi sinh embedding: {e}"}
        )