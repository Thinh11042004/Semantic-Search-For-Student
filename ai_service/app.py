import os
import logging
import unicodedata  # h·ªó tr·ª£ chu·∫©n h√≥a ti·∫øng Vi·ªát kh√¥ng d·∫•u
from fastapi import FastAPI, Body, Query
from fastapi.responses import JSONResponse
from sentence_transformers import SentenceTransformer
import psycopg2
from PyPDF2 import PdfReader
import docx 
from typing import Union, List
import numpy as np
import re

# T·∫°o th∆∞ m·ª•c logs n·∫øu ch∆∞a c√≥
if not os.path.exists("logs"):
    os.makedirs("logs")

# Thi·∫øt l·∫≠p ghi log
logging.basicConfig(
    filename="logs/upload.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = FastAPI()
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  # 768 chi·ªÅu 

# K·∫øt n·ªëi PostgreSQL
conn = psycopg2.connect(
    host="semantic_search_db",  
    port=5432,
    user="admin",
    password="123456",
    database="StudentFormDB"
)
cursor = conn.cursor()

# Helper function ƒë·ªÉ ƒë·ªçc file PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text
    return text

# Helper function ƒë·ªÉ ƒë·ªçc file DOCX
def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

# Chu·∫©n h√≥a ti·∫øng Vi·ªát (b·ªè d·∫•u, lowercase)
def normalize_text(text):
    """
    ‚úÖ Chu·∫©n h√≥a ti·∫øng Vi·ªát: b·ªè d·∫•u, chuy·ªÉn v·ªÅ lowercase
    """
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    return text.lower()

# üîß M·ªöI: Mapping t·ª´ vi·∫øt t·∫Øt ho·∫∑c ti·∫øng Anh ph·ªï bi·∫øn
SYNONYM_MAP = {
    "hocbong": "h·ªçc b·ªïng",
    "hoclai": "h·ªçc l·∫°i",
    "nghihoc": "ngh·ªâ h·ªçc",
    "report": "t∆∞·ªùng tr√¨nh",
    "dt": "ƒëi·ªán tho·∫°i",
    "sv": "sinh vi√™n",
    "gv": "gi·∫£ng vi√™n",
    "cancel": "h·ªßy",
    "apply": "xin",
    "form": "bi·ªÉu m·∫´u",
}

def normalize_and_expand(text):
    """
    ‚úÖ Chu·∫©n h√≥a + x·ª≠ l√Ω vi·∫øt li·ªÅn + d·ªãch vi·∫øt t·∫Øt/ti·∫øng Anh
    """
    text = normalize_text(text.replace(" ", ""))
    return SYNONYM_MAP.get(text, text)

# H√†m ki·ªÉm tra xem t√™n file ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu ch∆∞a
def is_file_exists(title):  
    cursor.execute("SELECT COUNT(*) FROM forms WHERE title = %s", (title,))
    count = cursor.fetchone()[0]
    return count > 0

# H√†m x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file
def delete_duplicate_files():
    try:
        cursor.execute("""
            DELETE FROM forms
            WHERE ctid NOT IN (
                SELECT min(ctid)
                FROM documents
                GROUP BY title
            );
        """)
        conn.commit()
        logging.info("ƒê√£ x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file.")
    except Exception as e:
        logging.error(f"L·ªói khi x√≥a file tr√πng: {e}")
        conn.rollback()

# H√†m upload file (d√πng chung cho API v√† auto load)
def upload_file_to_db(file_path):
    title = os.path.basename(file_path)
    
    if is_file_exists(title):
        logging.info(f"File '{title}' ƒë√£ t·ªìn t·∫°i. X√≥a file tr√πng...")
        delete_duplicate_files()
    
    content = ""
    if file_path.endswith(".pdf"):
        content = extract_text_from_pdf(file_path)
    elif file_path.endswith(".docx"):
        content = extract_text_from_docx(file_path)
    else:
        logging.warning(f"B·ªè qua file kh√¥ng h·ªó tr·ª£: {file_path}")
        return {"error": "Unsupported file type"}
    
    embedding = model.encode(content).tolist()
    
    try:
        cursor.execute(
            "INSERT INTO forms (title, content, embedding) VALUES (%s, %s, %s)",
            (title, content, embedding)
        )
        conn.commit()
        logging.info(f"Uploaded: {title}")
        return {"status": "uploaded", "title": title}
    except Exception as e:
        logging.error(f"L·ªói khi upload file: {e}")
        conn.rollback()
        return {"error": f"Failed to upload file: {e}"}

#API/ Search
@app.get("/search")
def semantic_search(
    query: str = Query(..., description="C√¢u truy v·∫•n t√¨m ki·∫øm"),
    top_k: int = Query(5, description="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ mu·ªën tr·∫£ v·ªÅ")
):
    """
    üîç Semantic search k·∫øt h·ª£p t·ª´ kh√≥a g·∫ßn ƒë√∫ng, vi·∫øt t·∫Øt, v√† vector ng·ªØ nghƒ©a SBERT.
    """
    logging.info(f"üîç Semantic search: {query}")
    try:
        # M√£ h√≥a truy v·∫•n th√†nh vector
        query_vec = model.encode(query).tolist()

        # Chu·∫©n h√≥a truy v·∫•n (kh√¥ng d·∫•u, vi·∫øt t·∫Øt, d·ªãch ti·∫øng Anh)
        query_clean = normalize_text(query)
        query_expanded = normalize_and_expand(query_clean)

        # L·∫•y to√†n b·ªô bi·ªÉu m·∫´u
        cursor.execute("SELECT id, title, content, embedding FROM forms")
        rows = cursor.fetchall()

        # T√≠nh cosine similarity
        def cosine_similarity(a, b):
            a = np.array(a, dtype=np.float32)
            b = np.array(eval(b), dtype=np.float32) if isinstance(b, str) else np.array(b, dtype=np.float32)
            return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

        results = []

        for _id, title, content, embedding in rows:
            title_clean = normalize_text(title)
            content_clean = normalize_text(content)
            full_text = f"{title_clean} {content_clean}"

            if query_expanded in title_clean:
                score = 1.0
                reason = "üéØ Kh·ªõp g·∫ßn ƒë√∫ng ti√™u ƒë·ªÅ (kh√¥ng d·∫•u)"
            elif query_expanded in full_text:
                score = 0.9
                reason = "üìö Kh·ªõp to√†n vƒÉn (chu·∫©n h√≥a kh√¥ng d·∫•u)"
            elif any(word in full_text for word in query_expanded.split()):
                score = 0.8
                reason = "üìå C√≥ t·ª´ li√™n quan trong n·ªôi dung"
            else:
                score = cosine_similarity(query_vec, embedding)
                reason = "ü§ñ Kh·ªõp ng·ªØ nghƒ©a (SBERT)"

            # T·∫°o snippet ph√π h·ª£p
            sentences = re.split(r'[.?!]\s+', content)
            matched = [s for s in sentences if query_expanded in normalize_text(s)]
            snippet = " ".join(matched)[:300] + "..." if matched else content[:300] + "..."

            results.append({
                "id": _id,
                "title": title,
                "snippet": snippet,
                "similarity_score": round(score, 4),
                "reason": reason
            })

        # Tr·∫£ v·ªÅ top-k theo score
        sorted_results = sorted(results, key=lambda x: x["similarity_score"], reverse=True)

        return {
            "query": query,
            "results": sorted_results[:top_k] 
        }

    except Exception as e:
        logging.error(f"‚ùå L·ªói t√¨m ki·∫øm: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": "L·ªói h·ªá th·ªëng", "detail": str(e)}
        )



@app.post("/top-k")
def top_k_search(query: str = Body(...), k: int = Body(...)):
    logging.info(f"üîç ƒêang t√¨m ki·∫øm v·ªõi truy v·∫•n: {query}, Top {k} k·∫øt qu·∫£")
    try:
        query_embedding = model.encode(query).tolist()
        cursor.execute("""
            SELECT title, content
            FROM forms
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_embedding, k))
        results = cursor.fetchall()
        if results:
            return {
                "query": query,
                "top_k_results": [
                    {"title": result[0], "content": result[1][:500] + "..."} for result in results
                ]
            }
        else:
            return {
                "query": query,
                "message": f"Kh√¥ng t√¨m th·∫•y {k} bi·ªÉu m·∫´u ph√π h·ª£p."
            }
    except Exception as e:
        logging.error(f"L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm: {e}")
        cursor.execute("ROLLBACK;")
        return {"error": f"Top-K search failed: {e}"}

@app.post("/get-embedding")
def get_embedding(text: Union[str, List[str]] = Body(...)):
    try:
        if isinstance(text, str):
            text = [text]
        text = [t[:5000] for t in text]
        embeddings = model.encode(text).tolist()
        return {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings
        }
    except Exception as e:
        logging.error(f"L·ªói khi sinh embedding: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"L·ªói khi sinh embedding: {e}"}
        )
